{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gulliyevn/ML-NLP-LLMProjects/blob/main/TraderPlusOnline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –Ø–ß–ï–ô–ö–ê 1: –£–°–¢–ê–ù–û–í–ö–ê –ë–ò–ë–õ–ò–û–¢–ï–ö –ò –ò–ú–ü–û–†–¢–´\n",
        "# ==================================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (LSTM, Dense, Dropout, Input,\n",
        "                                   BatchNormalization, MultiHeadAttention,\n",
        "                                   LayerNormalization, Add, GlobalAveragePooling1D)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "plt.style.use('dark_background')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"üöÄ LSTM FOREX PREDICTION SYSTEM\")\n",
        "print(\"=\"*50)\n",
        "print(\"‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!\")\n",
        "print(f\"üîß TensorFlow –≤–µ—Ä—Å–∏—è: {tf.__version__}\")\n",
        "print(f\"üîß GPU –¥–æ—Å—Ç—É–ø–Ω–æ: {len(tf.config.experimental.list_physical_devices('GPU'))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8hS1upfdPWI",
        "outputId": "83cc879b-06ac-4786-a22e-468f7f958f03"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ LSTM FOREX PREDICTION SYSTEM\n",
            "==================================================\n",
            "‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!\n",
            "üîß TensorFlow –≤–µ—Ä—Å–∏—è: 2.18.0\n",
            "üîß GPU –¥–æ—Å—Ç—É–ø–Ω–æ: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –Ø–ß–ï–ô–ö–ê 2: –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• EUR/USD 15M\n",
        "# ==================================================================================\n",
        "\n",
        "def load_forex_data():\n",
        "    \"\"\"\n",
        "    –ó–∞–≥—Ä—É–∑–∫–∞ EUR/USD –¥–∞–Ω–Ω—ã—Ö –Ω–∞ 15-–º–∏–Ω—É—Ç–Ω–æ–º —Ç–∞–π–º—Ñ—Ä–µ–π–º–µ\n",
        "    \"\"\"\n",
        "    print(\"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º EUR/USD –¥–∞–Ω–Ω—ã–µ (15-–º–∏–Ω—É—Ç–Ω—ã–π —Ç–∞–π–º—Ñ—Ä–µ–π–º)...\")\n",
        "\n",
        "    try:\n",
        "        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 60 –¥–Ω–µ–π (15m –∏–Ω—Ç–µ—Ä–≤–∞–ª)\n",
        "        data = yf.download(\n",
        "            'EURUSD=X',\n",
        "            period='60d',\n",
        "            interval='15m',\n",
        "            progress=False,\n",
        "            auto_adjust=True,\n",
        "            prepost=False\n",
        "        )\n",
        "\n",
        "        if data.empty:\n",
        "            print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ\")\n",
        "            return None\n",
        "\n",
        "        # –û—á–∏—Å—Ç–∫–∞ –∫–æ–ª–æ–Ω–æ–∫\n",
        "        if isinstance(data.columns, pd.MultiIndex):\n",
        "            data.columns = [col[0] for col in data.columns]\n",
        "\n",
        "        # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏\n",
        "        data = data.dropna()\n",
        "\n",
        "        # –ë–∞–∑–æ–≤—ã–µ —Ä–∞—Å—á–µ—Ç—ã\n",
        "        data['Returns'] = data['Close'].pct_change()\n",
        "        data['HL_Range'] = (data['High'] - data['Low']) / data['Close']\n",
        "\n",
        "        print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data):,} –±–∞—Ä–æ–≤ EUR/USD\")\n",
        "        print(f\"üìÖ –ü–µ—Ä–∏–æ–¥: {data.index[0].strftime('%Y-%m-%d %H:%M')} - {data.index[-1].strftime('%Y-%m-%d %H:%M')}\")\n",
        "        print(f\"üíπ –¶–µ–Ω–æ–≤–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω: {data['Close'].min():.5f} - {data['Close'].max():.5f}\")\n",
        "\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}\")\n",
        "        return None\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "forex_data = load_forex_data()\n",
        "\n",
        "if forex_data is not None:\n",
        "    print(f\"\\nüìä –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "    print(forex_data.head())\n",
        "    print(f\"\\nüìã –ö–æ–ª–æ–Ω–∫–∏: {list(forex_data.columns)}\")\n",
        "else:\n",
        "    print(\"‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fq_FF-g3dXHf",
        "outputId": "4431abbb-35b2-4b50-f4c7-7f90f6d4c16f"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• –ó–∞–≥—Ä—É–∂–∞–µ–º EUR/USD –¥–∞–Ω–Ω—ã–µ (15-–º–∏–Ω—É—Ç–Ω—ã–π —Ç–∞–π–º—Ñ—Ä–µ–π–º)...\n",
            "‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ 5,653 –±–∞—Ä–æ–≤ EUR/USD\n",
            "üìÖ –ü–µ—Ä–∏–æ–¥: 2025-03-26 00:00 - 2025-06-17 15:00\n",
            "üíπ –¶–µ–Ω–æ–≤–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω: 1.07377 - 1.16320\n",
            "\n",
            "üìä –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:\n",
            "                              Close      High       Low      Open  Volume  \\\n",
            "Datetime                                                                    \n",
            "2025-03-26 00:00:00+00:00  1.079331  1.079331  1.079098  1.079098       0   \n",
            "2025-03-26 00:15:00+00:00  1.079331  1.079564  1.079214  1.079331       0   \n",
            "2025-03-26 00:30:00+00:00  1.078981  1.079447  1.078981  1.079331       0   \n",
            "2025-03-26 00:45:00+00:00  1.078865  1.078981  1.078749  1.078981       0   \n",
            "2025-03-26 01:00:00+00:00  1.078516  1.078632  1.078167  1.078516       0   \n",
            "\n",
            "                            Returns  HL_Range  \n",
            "Datetime                                       \n",
            "2025-03-26 00:00:00+00:00       NaN  0.000216  \n",
            "2025-03-26 00:15:00+00:00  0.000000  0.000324  \n",
            "2025-03-26 00:30:00+00:00 -0.000324  0.000432  \n",
            "2025-03-26 00:45:00+00:00 -0.000108  0.000216  \n",
            "2025-03-26 01:00:00+00:00 -0.000324  0.000431  \n",
            "\n",
            "üìã –ö–æ–ª–æ–Ω–∫–∏: ['Close', 'High', 'Low', 'Open', 'Volume', 'Returns', 'HL_Range']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –Ø–ß–ï–ô–ö–ê 3: –£–õ–£–ß–®–ï–ù–ù–´–ï –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –ò–ù–î–ò–ö–ê–¢–û–†–´\n",
        "# ==================================================================================\n",
        "\n",
        "def calculate_enhanced_technical_indicators(data):\n",
        "    \"\"\"\n",
        "    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
        "    \"\"\"\n",
        "    print(\"üîß –†–∞—Å—á–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...\")\n",
        "\n",
        "    df = data.copy()\n",
        "    high = df['High']\n",
        "    low = df['Low']\n",
        "    close = df['Close']\n",
        "    volume = df.get('Volume', pd.Series(index=df.index, data=1))  # –ï—Å–ª–∏ –Ω–µ—Ç –æ–±—ä–µ–º–∞\n",
        "\n",
        "    # 1. –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ (—É–ª—É—á—à–µ–Ω–Ω—ã–µ)\n",
        "    df['SMA_9'] = close.rolling(window=9).mean()\n",
        "    df['SMA_21'] = close.rolling(window=21).mean()\n",
        "    df['EMA_12'] = close.ewm(span=12).mean()\n",
        "    df['EMA_26'] = close.ewm(span=26).mean()\n",
        "\n",
        "    # –ö—Ä–æ—Å—Å—ã MA\n",
        "    df['MA_Cross'] = np.where(df['SMA_9'] > df['SMA_21'], 1, -1)\n",
        "    df['EMA_Cross'] = np.where(df['EMA_12'] > df['EMA_26'], 1, -1)\n",
        "\n",
        "    # 2. RSI (—É–ª—É—á—à–µ–Ω–Ω—ã–π)\n",
        "    delta = close.diff()\n",
        "    gain = delta.where(delta > 0, 0)\n",
        "    loss = -delta.where(delta < 0, 0)\n",
        "    avg_gain = gain.rolling(window=14).mean()\n",
        "    avg_loss = loss.rolling(window=14).mean()\n",
        "    rs = avg_gain / avg_loss\n",
        "    df['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    # RSI –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è –∏ —É—Ä–æ–≤–Ω–∏\n",
        "    df['RSI_Oversold'] = np.where(df['RSI'] < 30, 1, 0)\n",
        "    df['RSI_Overbought'] = np.where(df['RSI'] > 70, 1, 0)\n",
        "    df['RSI_Momentum'] = df['RSI'].diff()\n",
        "\n",
        "    # 3. MACD (—É–ª—É—á—à–µ–Ω–Ω—ã–π)\n",
        "    ema_fast = close.ewm(span=12).mean()\n",
        "    ema_slow = close.ewm(span=26).mean()\n",
        "    df['MACD'] = ema_fast - ema_slow\n",
        "    df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()\n",
        "    df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']\n",
        "    df['MACD_Cross'] = np.where(df['MACD'] > df['MACD_Signal'], 1, -1)\n",
        "\n",
        "    # 4. Bollinger Bands (—É–ª—É—á—à–µ–Ω–Ω—ã–µ)\n",
        "    sma_20 = close.rolling(window=20).mean()\n",
        "    std_20 = close.rolling(window=20).std()\n",
        "    bb_upper = sma_20 + (2 * std_20)\n",
        "    bb_lower = sma_20 - (2 * std_20)\n",
        "    df['BB_Position'] = (close - bb_lower) / (bb_upper - bb_lower)\n",
        "    df['BB_Width'] = (bb_upper - bb_lower) / sma_20\n",
        "    df['BB_Squeeze'] = np.where(df['BB_Width'] < df['BB_Width'].rolling(20).mean(), 1, 0)\n",
        "\n",
        "    # 5. ATR (—É–ª—É—á—à–µ–Ω–Ω—ã–π)\n",
        "    tr1 = high - low\n",
        "    tr2 = abs(high - close.shift(1))\n",
        "    tr3 = abs(low - close.shift(1))\n",
        "    true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
        "    df['ATR'] = true_range.rolling(window=14).mean()\n",
        "    df['ATR_Percent'] = df['ATR'] / close * 100\n",
        "    df['ATR_Ratio'] = df['ATR'] / df['ATR'].rolling(50).mean()\n",
        "\n",
        "    # 6. ADX - –¢—Ä–µ–Ω–¥/–§–ª–µ—Ç (–ò–ó –î–û–ö–£–ú–ï–ù–¢–ê!)\n",
        "    def calculate_adx(high, low, close, period=14):\n",
        "        plus_dm = high.diff()\n",
        "        minus_dm = low.diff()\n",
        "        plus_dm[plus_dm < 0] = 0\n",
        "        minus_dm[minus_dm > 0] = 0\n",
        "\n",
        "        tr = pd.concat([high - low,\n",
        "                       abs(high - close.shift(1)),\n",
        "                       abs(low - close.shift(1))], axis=1).max(axis=1)\n",
        "\n",
        "        atr = tr.rolling(period).mean()\n",
        "        plus_di = 100 * (plus_dm.rolling(period).mean() / atr)\n",
        "        minus_di = 100 * (minus_dm.abs().rolling(period).mean() / atr)\n",
        "\n",
        "        dx = 100 * abs(plus_di - minus_di) / (plus_di + minus_di)\n",
        "        adx = dx.rolling(period).mean()\n",
        "\n",
        "        return adx, plus_di, minus_di\n",
        "\n",
        "    df['ADX'], df['DI_Plus'], df['DI_Minus'] = calculate_adx(high, low, close)\n",
        "    df['ADX_Trend'] = np.where(df['ADX'] > 25, 1, 0)  # 1 = —Ç—Ä–µ–Ω–¥, 0 = —Ñ–ª–µ—Ç\n",
        "\n",
        "    # 7. Williams %R (–ò–ó –î–û–ö–£–ú–ï–ù–¢–ê!)\n",
        "    df['Williams_R'] = -100 * (high.rolling(14).max() - close) / (high.rolling(14).max() - low.rolling(14).min())\n",
        "    df['Williams_Oversold'] = np.where(df['Williams_R'] > -20, 1, 0)\n",
        "    df['Williams_Overbought'] = np.where(df['Williams_R'] < -80, 1, 0)\n",
        "\n",
        "    # 8. CCI - Commodity Channel Index (–ò–ó –î–û–ö–£–ú–ï–ù–¢–ê!)\n",
        "    typical_price = (high + low + close) / 3\n",
        "    sma_tp = typical_price.rolling(20).mean()\n",
        "    mad = typical_price.rolling(20).apply(lambda x: abs(x - x.mean()).mean())\n",
        "    df['CCI'] = (typical_price - sma_tp) / (0.015 * mad)\n",
        "    df['CCI_Extreme'] = np.where(abs(df['CCI']) > 100, 1, 0)\n",
        "\n",
        "    # 9. Stochastic Oscillator (–ò–ó –î–û–ö–£–ú–ï–ù–¢–ê!)\n",
        "    low_14 = low.rolling(14).min()\n",
        "    high_14 = high.rolling(14).max()\n",
        "    df['Stoch_K'] = 100 * (close - low_14) / (high_14 - low_14)\n",
        "    df['Stoch_D'] = df['Stoch_K'].rolling(3).mean()\n",
        "    df['Stoch_Cross'] = np.where(df['Stoch_K'] > df['Stoch_D'], 1, -1)\n",
        "\n",
        "    # 10. –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
        "    df['Volatility_5'] = close.pct_change().rolling(5).std() * 100\n",
        "    df['Volatility_20'] = close.pct_change().rolling(20).std() * 100\n",
        "    df['Vol_Ratio'] = df['Volatility_5'] / df['Volatility_20']\n",
        "\n",
        "    # 11. Momentum –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\n",
        "    df['ROC_5'] = (close - close.shift(5)) / close.shift(5) * 100\n",
        "    df['ROC_10'] = (close - close.shift(10)) / close.shift(10) * 100\n",
        "    df['Momentum'] = close / close.shift(10) * 100\n",
        "\n",
        "    # 12. –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—É–ª—É—á—à–µ–Ω–Ω—ã–µ)\n",
        "    df['Hour'] = df.index.hour\n",
        "    df['DayOfWeek'] = df.index.dayofweek\n",
        "    df['IsLondonSession'] = np.where((df['Hour'] >= 8) & (df['Hour'] <= 16), 1, 0)\n",
        "    df['IsNYSession'] = np.where((df['Hour'] >= 13) & (df['Hour'] <= 21), 1, 0)\n",
        "    df['IsOverlap'] = np.where((df['Hour'] >= 13) & (df['Hour'] <= 16), 1, 0)\n",
        "\n",
        "    # 13. –õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ)\n",
        "    for lag in [1, 2, 3, 4, 5]:\n",
        "        df[f'Returns_lag_{lag}'] = df['Returns'].shift(lag)\n",
        "        df[f'RSI_lag_{lag}'] = df['RSI'].shift(lag)\n",
        "        df[f'MACD_lag_{lag}'] = df['MACD'].shift(lag)\n",
        "\n",
        "    # 14. –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ (–ù–û–í–û–ï!)\n",
        "    df['RSI_MACD_Interaction'] = df['RSI'] * df['MACD_Cross']\n",
        "    df['BB_RSI_Interaction'] = df['BB_Position'] * df['RSI'] / 100\n",
        "    df['ADX_RSI_Strength'] = df['ADX'] * df['RSI'] / 100\n",
        "    df['Vol_Momentum'] = df['Vol_Ratio'] * df['ROC_5']\n",
        "\n",
        "    # 15. –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã\n",
        "    bullish_signals = (\n",
        "        (df['RSI'] < 30).astype(int) +\n",
        "        (df['Williams_R'] < -80).astype(int) +\n",
        "        (df['MACD_Cross'] == 1).astype(int) +\n",
        "        (df['MA_Cross'] == 1).astype(int) +\n",
        "        (df['Stoch_Cross'] == 1).astype(int)\n",
        "    )\n",
        "    df['Bullish_Cluster'] = bullish_signals\n",
        "\n",
        "    bearish_signals = (\n",
        "        (df['RSI'] > 70).astype(int) +\n",
        "        (df['Williams_R'] > -20).astype(int) +\n",
        "        (df['MACD_Cross'] == -1).astype(int) +\n",
        "        (df['MA_Cross'] == -1).astype(int) +\n",
        "        (df['Stoch_Cross'] == -1).astype(int)\n",
        "    )\n",
        "    df['Bearish_Cluster'] = bearish_signals\n",
        "\n",
        "    print(f\"‚úÖ –†–∞—Å—Å—á–∏—Ç–∞–Ω–æ {len([col for col in df.columns if col not in ['Open', 'High', 'Low', 'Close', 'Volume']])} –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤\")\n",
        "    print(f\"üìä –î–æ–±–∞–≤–ª–µ–Ω–æ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞: ADX, Williams %R, CCI, Stochastic\")\n",
        "    print(f\"üî• –ù–æ–≤—ã–µ —Ñ–∏—á–∏: –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è, –∫–ª–∞—Å—Ç–µ—Ä—ã, —Å–µ—Å—Å–∏–∏\")\n",
        "\n",
        "    return df\n",
        "\n",
        "    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\n",
        "if forex_data is not None:\n",
        "    forex_with_indicators = calculate_enhanced_technical_indicators(forex_data)  # ‚Üê –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø\n",
        "    print(f\"\\nüìä –î–∞–Ω–Ω—ã–µ —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏: {forex_with_indicators.shape}\")\n",
        "    print(f\"üìã –í—Å–µ –∫–æ–ª–æ–Ω–∫–∏: {list(forex_with_indicators.columns)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx46wNtRdcjK",
        "outputId": "2ad2d8e4-f4de-4f7b-8c40-824506b4c94f"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß –†–∞—Å—á–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...\n",
            "‚úÖ –†–∞—Å—Å—á–∏—Ç–∞–Ω–æ 66 –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤\n",
            "üìä –î–æ–±–∞–≤–ª–µ–Ω–æ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞: ADX, Williams %R, CCI, Stochastic\n",
            "üî• –ù–æ–≤—ã–µ —Ñ–∏—á–∏: –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è, –∫–ª–∞—Å—Ç–µ—Ä—ã, —Å–µ—Å—Å–∏–∏\n",
            "\n",
            "üìä –î–∞–Ω–Ω—ã–µ —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏: (5653, 71)\n",
            "üìã –í—Å–µ –∫–æ–ª–æ–Ω–∫–∏: ['Close', 'High', 'Low', 'Open', 'Volume', 'Returns', 'HL_Range', 'SMA_9', 'SMA_21', 'EMA_12', 'EMA_26', 'MA_Cross', 'EMA_Cross', 'RSI', 'RSI_Oversold', 'RSI_Overbought', 'RSI_Momentum', 'MACD', 'MACD_Signal', 'MACD_Histogram', 'MACD_Cross', 'BB_Position', 'BB_Width', 'BB_Squeeze', 'ATR', 'ATR_Percent', 'ATR_Ratio', 'ADX', 'DI_Plus', 'DI_Minus', 'ADX_Trend', 'Williams_R', 'Williams_Oversold', 'Williams_Overbought', 'CCI', 'CCI_Extreme', 'Stoch_K', 'Stoch_D', 'Stoch_Cross', 'Volatility_5', 'Volatility_20', 'Vol_Ratio', 'ROC_5', 'ROC_10', 'Momentum', 'Hour', 'DayOfWeek', 'IsLondonSession', 'IsNYSession', 'IsOverlap', 'Returns_lag_1', 'RSI_lag_1', 'MACD_lag_1', 'Returns_lag_2', 'RSI_lag_2', 'MACD_lag_2', 'Returns_lag_3', 'RSI_lag_3', 'MACD_lag_3', 'Returns_lag_4', 'RSI_lag_4', 'MACD_lag_4', 'Returns_lag_5', 'RSI_lag_5', 'MACD_lag_5', 'RSI_MACD_Interaction', 'BB_RSI_Interaction', 'ADX_RSI_Strength', 'Vol_Momentum', 'Bullish_Cluster', 'Bearish_Cluster']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –Ø–ß–ï–ô–ö–ê 4: –°–û–ó–î–ê–ù–ò–ï –¶–ï–õ–ï–í–û–ô –ü–ï–†–ï–ú–ï–ù–ù–û–ô\n",
        "# ==================================================================================\n",
        "\n",
        "def create_target_variable(data, forecast_horizon=4):\n",
        "    \"\"\"\n",
        "    –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "    forecast_horizon=4 –æ–∑–Ω–∞—á–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 1 —á–∞—Å –≤–ø–µ—Ä–µ–¥ (4 * 15min)\n",
        "    \"\"\"\n",
        "    print(f\"üéØ –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (–ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ {forecast_horizon * 15} –º–∏–Ω—É—Ç)...\")\n",
        "\n",
        "    df = data.copy()\n",
        "\n",
        "    # –ë—É–¥—É—â–∏–µ —Ü–µ–Ω—ã\n",
        "    future_close = df['Close'].shift(-forecast_horizon)\n",
        "    current_close = df['Close']\n",
        "\n",
        "    # –†–∞—Å—á–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã\n",
        "    price_change = (future_close - current_close) / current_close\n",
        "\n",
        "    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è\n",
        "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º 0.5 * ATR –∫–∞–∫ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–∏–º–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ\n",
        "    min_movement = df['ATR_Percent'] / 100 * 0.5\n",
        "\n",
        "    target = np.where(\n",
        "        price_change > min_movement, 1,  # BUY\n",
        "        np.where(price_change < -min_movement, -1,  # SELL\n",
        "                0)  # HOLD\n",
        "    )\n",
        "\n",
        "    df['Target'] = target\n",
        "    df['Price_Change'] = price_change\n",
        "\n",
        "    # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π\n",
        "    df = df[:-forecast_horizon].copy()\n",
        "\n",
        "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
        "    target_counts = pd.Series(target[~np.isnan(target)]).value_counts().sort_index()\n",
        "    total = len(target[~np.isnan(target)])\n",
        "\n",
        "    print(f\"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:\")\n",
        "    labels = {-1: 'SELL', 0: 'HOLD', 1: 'BUY'}\n",
        "    for value, count in target_counts.items():\n",
        "        if not np.isnan(value):\n",
        "            label = labels.get(int(value), f'Unknown({value})')\n",
        "            percentage = count / total * 100\n",
        "            print(f\"   {label}: {count:,} ({percentage:.1f}%)\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é\n",
        "if 'forex_with_indicators' in locals():\n",
        "    forex_final = create_target_variable(forex_with_indicators)\n",
        "    print(f\"\\nüìä –§–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {forex_final.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9y5AZAw1dfnP",
        "outputId": "aba5b78c-7d64-49db-9db2-ca6dd9310845"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (–ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 60 –º–∏–Ω—É—Ç)...\n",
            "üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:\n",
            "   SELL: 1,856 (32.8%)\n",
            "   HOLD: 1,710 (30.2%)\n",
            "   BUY: 2,087 (36.9%)\n",
            "\n",
            "üìä –§–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: (5649, 73)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –Ø–ß–ï–ô–ö–ê 5: –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ü–û–î–ì–û–¢–û–í–ö–ê –ü–†–ò–ó–ù–ê–ö–û–í –î–õ–Ø LSTM\n",
        "# ==================================================================================\n",
        "\n",
        "def prepare_enhanced_features_for_lstm(data):\n",
        "    \"\"\"\n",
        "    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è LSTM –º–æ–¥–µ–ª–∏\n",
        "    \"\"\"\n",
        "    print(\"üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è LSTM...\")\n",
        "\n",
        "    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "    core_features = [\n",
        "        'Close', 'High', 'Low', 'Open', 'Returns', 'HL_Range'\n",
        "    ]\n",
        "\n",
        "    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (–±–∞–∑–æ–≤—ã–µ)\n",
        "    technical_features = [\n",
        "        'SMA_9', 'SMA_21', 'EMA_12', 'EMA_26', 'RSI', 'MACD', 'MACD_Signal',\n",
        "        'MACD_Histogram', 'BB_Position', 'BB_Width', 'ATR_Percent', 'ATR_Ratio'\n",
        "    ]\n",
        "\n",
        "    # –ù–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
        "    advanced_features = [\n",
        "        'ADX', 'DI_Plus', 'DI_Minus', 'Williams_R', 'CCI', 'Stoch_K', 'Stoch_D',\n",
        "        'Volatility_5', 'Volatility_20', 'Vol_Ratio', 'ROC_5', 'ROC_10', 'Momentum'\n",
        "    ]\n",
        "\n",
        "    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "    time_features = [\n",
        "        'Hour', 'DayOfWeek', 'IsLondonSession', 'IsNYSession', 'IsOverlap'\n",
        "    ]\n",
        "\n",
        "    # –ö—Ä–æ—Å—Å—ã –∏ —Å–∏–≥–Ω–∞–ª—ã\n",
        "    signal_features = [\n",
        "        'MA_Cross', 'EMA_Cross', 'MACD_Cross', 'Stoch_Cross', 'ADX_Trend',\n",
        "        'RSI_Oversold', 'RSI_Overbought', 'Williams_Oversold', 'Williams_Overbought',\n",
        "        'CCI_Extreme', 'BB_Squeeze'\n",
        "    ]\n",
        "\n",
        "    # –õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "    lag_features = [\n",
        "        'Returns_lag_1', 'Returns_lag_2', 'Returns_lag_3', 'Returns_lag_4', 'Returns_lag_5',\n",
        "        'RSI_lag_1', 'RSI_lag_2', 'RSI_lag_3', 'MACD_lag_1', 'MACD_lag_2'\n",
        "    ]\n",
        "\n",
        "    # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è\n",
        "    interaction_features = [\n",
        "        'RSI_MACD_Interaction', 'BB_RSI_Interaction', 'ADX_RSI_Strength', 'Vol_Momentum'\n",
        "    ]\n",
        "\n",
        "    # –ö–ª–∞—Å—Ç–µ—Ä—ã\n",
        "    cluster_features = [\n",
        "        'Bullish_Cluster', 'Bearish_Cluster'\n",
        "    ]\n",
        "\n",
        "    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "    all_features = (core_features + technical_features + advanced_features +\n",
        "                   time_features + signal_features + lag_features +\n",
        "                   interaction_features + cluster_features)\n",
        "\n",
        "    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "    available_features = [col for col in all_features if col in data.columns]\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "    X = data[available_features].copy()\n",
        "    y = data['Target'].values\n",
        "\n",
        "    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN\n",
        "    mask = ~(np.isnan(y) | X.isnull().any(axis=1))\n",
        "    X_clean = X[mask].fillna(method='ffill').fillna(0)\n",
        "    y_clean = y[mask]\n",
        "\n",
        "    print(f\"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(available_features)} (–±—ã–ª–æ 19)\")\n",
        "    print(f\"üìä –ß–∏—Å—Ç—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: {len(X_clean):,}\")\n",
        "\n",
        "    # –†–∞–∑–±–∏–≤–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n",
        "    feature_categories = {\n",
        "        '–û—Å–Ω–æ–≤–Ω—ã–µ': [f for f in core_features if f in available_features],\n",
        "        '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ': [f for f in technical_features if f in available_features],\n",
        "        '–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ': [f for f in advanced_features if f in available_features],\n",
        "        '–í—Ä–µ–º–µ–Ω–Ω—ã–µ': [f for f in time_features if f in available_features],\n",
        "        '–°–∏–≥–Ω–∞–ª—ã': [f for f in signal_features if f in available_features],\n",
        "        '–õ–∞–≥–∏': [f for f in lag_features if f in available_features],\n",
        "        '–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è': [f for f in interaction_features if f in available_features],\n",
        "        '–ö–ª–∞—Å—Ç–µ—Ä—ã': [f for f in cluster_features if f in available_features]\n",
        "    }\n",
        "\n",
        "    print(f\"\\nüìã –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:\")\n",
        "    for category, features in feature_categories.items():\n",
        "        if features:\n",
        "            print(f\"   {category}: {len(features)} ({', '.join(features[:3])}{'...' if len(features) > 3 else ''})\")\n",
        "\n",
        "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Å–æ–≤\n",
        "    unique, counts = np.unique(y_clean, return_counts=True)\n",
        "    print(f\"\\nüìä –ò—Ç–æ–≥–æ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:\")\n",
        "    labels = {-1: 'SELL', 0: 'HOLD', 1: 'BUY'}\n",
        "    for val, count in zip(unique, counts):\n",
        "        if not np.isnan(val):\n",
        "            print(f\"   {labels.get(int(val))}: {count:,} ({count/len(y_clean)*100:.1f}%)\")\n",
        "\n",
        "    return X_clean, y_clean, available_features, feature_categories\n",
        "\n",
        "    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "if 'forex_final' in locals():\n",
        "    X, y, features, categories = prepare_enhanced_features_for_lstm(forex_final)\n",
        "    print(f\"\\n‚úÖ –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã –¥–ª—è LSTM: X{X.shape}, y{y.shape}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è forex_final –Ω–µ –Ω–∞–π–¥–µ–Ω. –í—ã–ø–æ–ª–Ω–∏—Ç–µ —è—á–µ–π–∫—É ‚Ññ4 —Å–Ω–∞—á–∞–ª–∞.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0hRHqz6pdgX7",
        "outputId": "f8a5da3d-f4ef-47ae-ecd6-527454e0650d"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è LSTM...\n",
            "‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: 63 (–±—ã–ª–æ 19)\n",
            "üìä –ß–∏—Å—Ç—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: 5,587\n",
            "\n",
            "üìã –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:\n",
            "   –û—Å–Ω–æ–≤–Ω—ã–µ: 6 (Close, High, Low...)\n",
            "   –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ: 12 (SMA_9, SMA_21, EMA_12...)\n",
            "   –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ: 13 (ADX, DI_Plus, DI_Minus...)\n",
            "   –í—Ä–µ–º–µ–Ω–Ω—ã–µ: 5 (Hour, DayOfWeek, IsLondonSession...)\n",
            "   –°–∏–≥–Ω–∞–ª—ã: 11 (MA_Cross, EMA_Cross, MACD_Cross...)\n",
            "   –õ–∞–≥–∏: 10 (Returns_lag_1, Returns_lag_2, Returns_lag_3...)\n",
            "   –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è: 4 (RSI_MACD_Interaction, BB_RSI_Interaction, ADX_RSI_Strength...)\n",
            "   –ö–ª–∞—Å—Ç–µ—Ä—ã: 2 (Bullish_Cluster, Bearish_Cluster)\n",
            "\n",
            "üìä –ò—Ç–æ–≥–æ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:\n",
            "   SELL: 1,837 (32.9%)\n",
            "   HOLD: 1,681 (30.1%)\n",
            "   BUY: 2,069 (37.0%)\n",
            "\n",
            "‚úÖ –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã –¥–ª—è LSTM: X(5587, 63), y(5587,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –Ø–ß–ï–ô–ö–ê 6: –£–õ–£–ß–®–ï–ù–ù–ê–Ø LSTM –ú–û–î–ï–õ–¨ –° ATTENTION (–ù–û–í–ê–Ø –í–ï–†–°–ò–Ø)\n",
        "# ==================================================================================\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "class EnhancedForexLSTMModel:\n",
        "    \"\"\"\n",
        "    –£–ª—É—á—à–µ–Ω–Ω–∞—è LSTM –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è forex —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏\n",
        "    –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö –∏–∑ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.history = None\n",
        "        self.attention_weights = None\n",
        "\n",
        "        # –£–õ–£–ß–®–ï–ù–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ –ú–û–î–ï–õ–ò (–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞)\n",
        "        self.sequence_length = 96  # 24 —á–∞—Å–∞ –∏—Å—Ç–æ—Ä–∏–∏ (96 * 15min) - –∫–∞–∫ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç\n",
        "        self.lstm_units = [128, 64, 32]  # 3 —Å–ª–æ—è –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
        "        self.dropout_rate = 0.3  # –ë–æ–ª—å—à–µ dropout –ø—Ä–æ—Ç–∏–≤ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è\n",
        "        self.recurrent_dropout = 0.2  # Dropout –¥–ª—è LSTM —Å–≤—è–∑–µ–π\n",
        "        self.attention_heads = 8  # –ë–æ–ª—å—à–µ –≥–æ–ª–æ–≤ attention –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è\n",
        "        self.learning_rate = 0.0001  # –ú–µ–Ω—å—à–µ LR –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
        "        self.batch_size = 32  # –ú–µ–Ω—å—à–µ batch –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
        "        self.epochs = 100  # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö\n",
        "\n",
        "        # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è\n",
        "        self.l2_reg = 0.001\n",
        "        self.patience = 15  # –ë–æ–ª—å—à–µ —Ç–µ—Ä–ø–µ–Ω–∏—è –¥–ª—è early stopping\n",
        "\n",
        "        # –ù–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
        "        self.use_class_weights = True\n",
        "        self.save_attention = True\n",
        "\n",
        "    def create_sequences(self, X, y):\n",
        "        \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è LSTM —Å –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏\"\"\"\n",
        "        print(f\"üîß –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª–∏–Ω–æ–π {self.sequence_length}...\")\n",
        "\n",
        "        if len(X) < self.sequence_length:\n",
        "            print(f\"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö! –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º {self.sequence_length}, –µ—Å—Ç—å {len(X)}\")\n",
        "            return None, None\n",
        "\n",
        "        X_sequences = []\n",
        "        y_sequences = []\n",
        "\n",
        "        for i in range(self.sequence_length, len(X)):\n",
        "            X_sequences.append(X[i-self.sequence_length:i])\n",
        "            y_sequences.append(y[i])\n",
        "\n",
        "        X_sequences = np.array(X_sequences, dtype=np.float32)\n",
        "        y_sequences = np.array(y_sequences)\n",
        "\n",
        "        print(f\"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(X_sequences):,} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π\")\n",
        "        print(f\"üìä –§–æ—Ä–º–∞ X: {X_sequences.shape}, —Ñ–æ—Ä–º–∞ y: {y_sequences.shape}\")\n",
        "\n",
        "        return X_sequences, y_sequences\n",
        "\n",
        "    def prepare_data(self, X, y):\n",
        "        \"\"\"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π\"\"\"\n",
        "        print(\"üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...\")\n",
        "\n",
        "        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º scaler\n",
        "        print(\"üìä –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...\")\n",
        "        self.scaler = RobustScaler()\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ scaler\n",
        "        os.makedirs('models', exist_ok=True)\n",
        "        with open('models/scaler.pkl', 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "        print(\"üíæ Scaler —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ models/scaler.pkl\")\n",
        "\n",
        "        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π\n",
        "        X_seq, y_seq = self.create_sequences(X_scaled, y)\n",
        "\n",
        "        if X_seq is None:\n",
        "            return None, None, None\n",
        "\n",
        "        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–≤–∞–∂–Ω–æ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤)\n",
        "        total_samples = len(X_seq)\n",
        "        test_size = int(total_samples * 0.15)  # 15% –Ω–∞ —Ç–µ—Å—Ç\n",
        "        val_size = int(total_samples * 0.15)   # 15% –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—é\n",
        "        train_size = total_samples - test_size - val_size\n",
        "\n",
        "        # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∞)\n",
        "        X_train = X_seq[:train_size]\n",
        "        y_train = y_seq[:train_size]\n",
        "        X_val = X_seq[train_size:train_size + val_size]\n",
        "        y_val = y_seq[train_size:train_size + val_size]\n",
        "        X_test = X_seq[train_size + val_size:]\n",
        "        y_test = y_seq[train_size + val_size:]\n",
        "\n",
        "        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (SELL=-1, HOLD=0, BUY=1 -> 0,1,2)\n",
        "        def to_categorical_custom(y_data):\n",
        "            y_cat = y_data + 1  # [-1,0,1] -> [0,1,2]\n",
        "            return tf.keras.utils.to_categorical(y_cat, num_classes=3)\n",
        "\n",
        "        y_train_cat = to_categorical_custom(y_train)\n",
        "        y_val_cat = to_categorical_custom(y_val)\n",
        "        y_test_cat = to_categorical_custom(y_test)\n",
        "\n",
        "        print(f\"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "        print(f\"   Train: {len(X_train):,} ({len(X_train)/total_samples*100:.1f}%)\")\n",
        "        print(f\"   Val: {len(X_val):,} ({len(X_val)/total_samples*100:.1f}%)\")\n",
        "        print(f\"   Test: {len(X_test):,} ({len(X_test)/total_samples*100:.1f}%)\")\n",
        "\n",
        "        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Å–æ–≤ –≤ –∫–∞–∂–¥–æ–º –Ω–∞–±–æ—Ä–µ\n",
        "        def print_class_distribution(y_data, dataset_name):\n",
        "            y_integers = np.argmax(y_data, axis=1)\n",
        "            unique, counts = np.unique(y_integers, return_counts=True)\n",
        "            class_names = ['SELL', 'HOLD', 'BUY']\n",
        "            print(f\"   {dataset_name}:\")\n",
        "            for i, (cls, count) in enumerate(zip(unique, counts)):\n",
        "                print(f\"     {class_names[cls]}: {count:,} ({count/len(y_data)*100:.1f}%)\")\n",
        "\n",
        "        print_class_distribution(y_train_cat, \"Train —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ\")\n",
        "        print_class_distribution(y_val_cat, \"Val —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ\")\n",
        "        print_class_distribution(y_test_cat, \"Test —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ\")\n",
        "\n",
        "        return (X_train, y_train_cat), (X_val, y_val_cat), (X_test, y_test_cat)\n",
        "\n",
        "    def build_model(self, input_shape):\n",
        "        \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π LSTM –º–æ–¥–µ–ª–∏ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π\"\"\"\n",
        "        print(\"üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π LSTM –º–æ–¥–µ–ª–∏...\")\n",
        "        print(f\"üìä –í—Ö–æ–¥–Ω–∞—è —Ñ–æ—Ä–º–∞: {input_shape}\")\n",
        "        print(f\"üîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: LSTM{self.lstm_units}, Attention heads={self.attention_heads}\")\n",
        "\n",
        "        # –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π\n",
        "        inputs = Input(shape=input_shape)\n",
        "\n",
        "        # 1-–π LSTM —Å–ª–æ–π\n",
        "        x = LSTM(self.lstm_units[0],\n",
        "                return_sequences=True,\n",
        "                dropout=self.dropout_rate,\n",
        "                recurrent_dropout=self.recurrent_dropout,\n",
        "                kernel_regularizer=l2(self.l2_reg))(inputs)\n",
        "        x = LayerNormalization()(x)\n",
        "\n",
        "        # 2-–π LSTM —Å–ª–æ–π\n",
        "        x = LSTM(self.lstm_units[1],\n",
        "                return_sequences=True,\n",
        "                dropout=self.dropout_rate,\n",
        "                recurrent_dropout=self.recurrent_dropout,\n",
        "                kernel_regularizer=l2(self.l2_reg))(x)\n",
        "        x = LayerNormalization()(x)\n",
        "\n",
        "        # 3-–π LSTM —Å–ª–æ–π\n",
        "        x = LSTM(self.lstm_units[2],\n",
        "                return_sequences=True,\n",
        "                dropout=self.dropout_rate,\n",
        "                recurrent_dropout=self.recurrent_dropout,\n",
        "                kernel_regularizer=l2(self.l2_reg))(x)\n",
        "        x = LayerNormalization()(x)\n",
        "\n",
        "        # –ü–µ—Ä–≤—ã–π Multi-Head Attention —Å–ª–æ–π\n",
        "        attention_output1 = MultiHeadAttention(\n",
        "            num_heads=self.attention_heads,\n",
        "            key_dim=64,\n",
        "            dropout=0.1\n",
        "        )(x, x)\n",
        "\n",
        "        # Residual connection + LayerNorm\n",
        "        x = Add()([x, attention_output1])\n",
        "        x = LayerNormalization()(x)\n",
        "\n",
        "        # –í—Ç–æ—Ä–æ–π Attention —Å–ª–æ–π (–º–µ–Ω—å—à–µ –≥–æ–ª–æ–≤)\n",
        "        attention_output2 = MultiHeadAttention(\n",
        "            num_heads=4,\n",
        "            key_dim=32,\n",
        "            dropout=0.1\n",
        "        )(x, x)\n",
        "\n",
        "        # –ï—â–µ –æ–¥–Ω–∞ residual connection\n",
        "        x = Add()([x, attention_output2])\n",
        "        x = LayerNormalization()(x)\n",
        "\n",
        "        # Global Average Pooling –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏\n",
        "        x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "        # Dense —Å–ª–æ–∏ —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π\n",
        "        x = Dense(64, activation='relu', kernel_regularizer=l2(self.l2_reg))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(self.dropout_rate)(x)\n",
        "\n",
        "        x = Dense(32, activation='relu', kernel_regularizer=l2(self.l2_reg))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(self.dropout_rate)(x)\n",
        "\n",
        "        x = Dense(16, activation='relu', kernel_regularizer=l2(self.l2_reg))(x)\n",
        "        x = Dropout(self.dropout_rate)(x)\n",
        "\n",
        "        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π\n",
        "        outputs = Dense(3, activation='softmax', name='predictions')(x)\n",
        "\n",
        "        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "        model = Model(inputs, outputs)\n",
        "\n",
        "        # –ö–∞—Å—Ç–æ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ F1, Precision, Recall\n",
        "        def precision_class(class_id):\n",
        "            def precision(y_true, y_pred):\n",
        "                y_true_class = tf.cast(tf.equal(tf.argmax(y_true, axis=1), class_id), tf.float32)\n",
        "                y_pred_class = tf.cast(tf.equal(tf.argmax(y_pred, axis=1), class_id), tf.float32)\n",
        "\n",
        "                tp = tf.reduce_sum(y_true_class * y_pred_class)\n",
        "                fp = tf.reduce_sum((1 - y_true_class) * y_pred_class)\n",
        "\n",
        "                return tp / (tp + fp + tf.keras.backend.epsilon())\n",
        "            return precision\n",
        "\n",
        "        def recall_class(class_id):\n",
        "            def recall(y_true, y_pred):\n",
        "                y_true_class = tf.cast(tf.equal(tf.argmax(y_true, axis=1), class_id), tf.float32)\n",
        "                y_pred_class = tf.cast(tf.equal(tf.argmax(y_pred, axis=1), class_id), tf.float32)\n",
        "\n",
        "                tp = tf.reduce_sum(y_true_class * y_pred_class)\n",
        "                fn = tf.reduce_sum(y_true_class * (1 - y_pred_class))\n",
        "\n",
        "                return tp / (tp + fn + tf.keras.backend.epsilon())\n",
        "            return recall\n",
        "\n",
        "        def f1_macro(y_true, y_pred):\n",
        "            def f1_score_class(class_id):\n",
        "                y_true_class = tf.cast(tf.equal(tf.argmax(y_true, axis=1), class_id), tf.float32)\n",
        "                y_pred_class = tf.cast(tf.equal(tf.argmax(y_pred, axis=1), class_id), tf.float32)\n",
        "\n",
        "                tp = tf.reduce_sum(y_true_class * y_pred_class)\n",
        "                fp = tf.reduce_sum((1 - y_true_class) * y_pred_class)\n",
        "                fn = tf.reduce_sum(y_true_class * (1 - y_pred_class))\n",
        "\n",
        "                precision = tp / (tp + fp + tf.keras.backend.epsilon())\n",
        "                recall = tp / (tp + fn + tf.keras.backend.epsilon())\n",
        "\n",
        "                return 2 * precision * recall / (precision + recall + tf.keras.backend.epsilon())\n",
        "\n",
        "            f1_0 = f1_score_class(0)  # SELL\n",
        "            f1_1 = f1_score_class(1)  # HOLD\n",
        "            f1_2 = f1_score_class(2)  # BUY\n",
        "\n",
        "            return (f1_0 + f1_1 + f1_2) / 3\n",
        "\n",
        "        # –ö–æ–º–ø–∏–ª—è—Ü–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º\n",
        "        model.compile(\n",
        "            optimizer=Adam(\n",
        "                learning_rate=self.learning_rate,\n",
        "                clipnorm=1.0,  # Gradient clipping\n",
        "                beta_1=0.9,\n",
        "                beta_2=0.999\n",
        "            ),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=[\n",
        "                'accuracy',\n",
        "                f1_macro,\n",
        "                precision_class(0),  # SELL precision\n",
        "                precision_class(1),  # HOLD precision\n",
        "                precision_class(2),  # BUY precision\n",
        "                recall_class(0),     # SELL recall\n",
        "                recall_class(1),     # HOLD recall\n",
        "                recall_class(2)      # BUY recall\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ –£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞!\")\n",
        "        print(f\"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {model.count_params():,}\")\n",
        "        print(f\"üîß –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {len(self.lstm_units)} LSTM —Å–ª–æ—è + 2 Attention —Å–ª–æ—è\")\n",
        "        print(f\"üìà –ú–µ—Ç—Ä–∏–∫–∏: Accuracy, F1-macro, Precision/Recall –ø–æ –∫–ª–∞—Å—Å–∞–º\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train(self, train_data, val_data):\n",
        "        \"\"\"–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ callback'–∞–º–∏\"\"\"\n",
        "        X_train, y_train = train_data\n",
        "        X_val, y_val = val_data\n",
        "\n",
        "        print(\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...\")\n",
        "        print(f\"‚è±Ô∏è –û–∂–∏–¥–∞–π—Ç–µ 20-30 –º–∏–Ω—É—Ç (—Å–ª–æ–∂–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)\")\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å\n",
        "        self.model = self.build_model((X_train.shape[1], X_train.shape[2]))\n",
        "\n",
        "        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ callbacks\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_f1_macro',\n",
        "                patience=self.patience,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1,\n",
        "                mode='max'\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=7,\n",
        "                verbose=1,\n",
        "                min_lr=1e-7\n",
        "            ),\n",
        "            ModelCheckpoint(\n",
        "                'models/best_enhanced_model.h5',\n",
        "                monitor='val_f1_macro',\n",
        "                save_best_only=True,\n",
        "                verbose=1,\n",
        "                mode='max'\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤\n",
        "        if self.use_class_weights:\n",
        "            y_integers = np.argmax(y_train, axis=1)\n",
        "            class_counts = np.bincount(y_integers)\n",
        "            total_samples = len(y_train)\n",
        "            class_weights = total_samples / (len(class_counts) * class_counts)\n",
        "            class_weight_dict = {i: class_weights[i] for i in range(len(class_counts))}\n",
        "\n",
        "            print(f\"‚öñÔ∏è –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏:\")\n",
        "            class_names = ['SELL', 'HOLD', 'BUY']\n",
        "            for i, weight in class_weight_dict.items():\n",
        "                print(f\"   {class_names[i]}: {weight:.3f}\")\n",
        "        else:\n",
        "            class_weight_dict = None\n",
        "\n",
        "        # –û–±—É—á–µ–Ω–∏–µ\n",
        "        print(f\"\\nüî• –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:\")\n",
        "        print(f\"   üìè Sequence length: {self.sequence_length}\")\n",
        "        print(f\"   üß† LSTM –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {self.lstm_units}\")\n",
        "        print(f\"   üëÅÔ∏è Attention heads: {self.attention_heads}\")\n",
        "        print(f\"   üìö Batch size: {self.batch_size}\")\n",
        "        print(f\"   üîÑ Max epochs: {self.epochs}\")\n",
        "        print(f\"   üéØ Learning rate: {self.learning_rate}\")\n",
        "\n",
        "        self.history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            batch_size=self.batch_size,\n",
        "            epochs=self.epochs,\n",
        "            validation_data=(X_val, y_val),\n",
        "            callbacks=callbacks,\n",
        "            class_weight=class_weight_dict,\n",
        "            verbose=1,\n",
        "            shuffle=False  # –ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤!\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n",
        "        return self.history\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        \"\"\"–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏\"\"\"\n",
        "        X_test, y_test = test_data\n",
        "\n",
        "        print(\"üìä –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏...\")\n",
        "\n",
        "        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "        y_pred_proba = self.model.predict(X_test, verbose=0)\n",
        "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "        y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
        "        test_results = self.model.evaluate(X_test, y_test, verbose=0)\n",
        "        metric_names = self.model.metrics_names\n",
        "\n",
        "        print(f\"üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "        for name, value in zip(metric_names, test_results):\n",
        "            if 'accuracy' in name:\n",
        "                print(f\"   {name}: {value:.4f} ({value*100:.2f}%)\")\n",
        "            else:\n",
        "                print(f\"   {name}: {value:.4f}\")\n",
        "\n",
        "        # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º\n",
        "        class_names = ['SELL', 'HOLD', 'BUY']\n",
        "        print(f\"\\nüìã –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º:\")\n",
        "        print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
        "\n",
        "        # –ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏\n",
        "        confidence = np.max(y_pred_proba, axis=1)\n",
        "        print(f\"\\nüéØ –ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏:\")\n",
        "        print(f\"   –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence.mean():.3f}\")\n",
        "        print(f\"   –ú–µ–¥–∏–∞–Ω–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {np.median(confidence):.3f}\")\n",
        "        print(f\"   –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (>0.8): {(confidence > 0.8).sum()/len(confidence)*100:.1f}%\")\n",
        "        print(f\"   –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (<0.5): {(confidence < 0.5).sum()/len(confidence)*100:.1f}%\")\n",
        "\n",
        "        return {\n",
        "            'accuracy': test_results[1],  # accuracy –º–µ—Ç—Ä–∏–∫–∞\n",
        "            'f1_macro': test_results[2],  # f1_macro –º–µ—Ç—Ä–∏–∫–∞\n",
        "            'loss': test_results[0],\n",
        "            'predictions': y_pred_proba,\n",
        "            'predictions_class': y_pred,\n",
        "            'true_class': y_true,\n",
        "            'confidence': confidence,\n",
        "            'confusion_matrix': confusion_matrix(y_true, y_pred),\n",
        "            'classification_report': classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "        }\n",
        "\n",
        "    def save_model(self, filepath='models/enhanced_forex_model'):\n",
        "        \"\"\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏\"\"\"\n",
        "        if self.model is not None:\n",
        "            self.model.save(f\"{filepath}.h5\")\n",
        "            print(f\"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filepath}.h5\")\n",
        "\n",
        "    def load_model(self, filepath='models/enhanced_forex_model.h5'):\n",
        "        \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏\"\"\"\n",
        "        if os.path.exists(filepath):\n",
        "            self.model = tf.keras.models.load_model(filepath)\n",
        "            print(f\"üìÇ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filepath}\")\n",
        "        else:\n",
        "            print(f\"‚ùå –§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}\")\n",
        "\n",
        "print(\"‚úÖ –ö–ª–∞—Å—Å EnhancedForexLSTMModel –≥–æ—Ç–æ–≤!\")\n",
        "print(\"üî• –£–ª—É—á—à–µ–Ω–∏—è:\")\n",
        "print(\"   ‚Ä¢ Sequence length —É–≤–µ–ª–∏—á–µ–Ω –¥–æ 96 (24 —á–∞—Å–∞)\")\n",
        "print(\"   ‚Ä¢ 3 LSTM —Å–ª–æ—è + 2 Attention —Å–ª–æ—è\")\n",
        "print(\"   ‚Ä¢ F1-score, Precision, Recall –º–µ—Ç—Ä–∏–∫–∏\")\n",
        "print(\"   ‚Ä¢ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ scaler –∏ –º–æ–¥–µ–ª–∏\")\n",
        "print(\"   ‚Ä¢ –õ—É—á—à–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –∏ class weights\")\n",
        "print(\"   ‚Ä¢ Gradient clipping –∏ —É–ª—É—á—à–µ–Ω–Ω—ã–π Adam\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb57UfHZdj6f",
        "outputId": "1617c689-7504-402f-a49f-df371564aeff"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ –ö–ª–∞—Å—Å EnhancedForexLSTMModel –≥–æ—Ç–æ–≤!\n",
            "üî• –£–ª—É—á—à–µ–Ω–∏—è:\n",
            "   ‚Ä¢ Sequence length —É–≤–µ–ª–∏—á–µ–Ω –¥–æ 96 (24 —á–∞—Å–∞)\n",
            "   ‚Ä¢ 3 LSTM —Å–ª–æ—è + 2 Attention —Å–ª–æ—è\n",
            "   ‚Ä¢ F1-score, Precision, Recall –º–µ—Ç—Ä–∏–∫–∏\n",
            "   ‚Ä¢ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ scaler –∏ –º–æ–¥–µ–ª–∏\n",
            "   ‚Ä¢ –õ—É—á—à–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –∏ class weights\n",
            "   ‚Ä¢ Gradient clipping –∏ —É–ª—É—á—à–µ–Ω–Ω—ã–π Adam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # –Ø–ß–ï–ô–ö–ê 7: –û–ë–£–ß–ï–ù–ò–ï –£–õ–£–ß–®–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò (–ù–û–í–ê–Ø –í–ï–†–°–ò–Ø)\n",
        "# # ==================================================================================\n",
        "\n",
        "# def train_enhanced_forex_model_v2(data):\n",
        "#     \"\"\"\n",
        "#     –û–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–π —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ EnhancedForexLSTMModel\n",
        "#     \"\"\"\n",
        "#     print(\"üöÄ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø ENHANCED LSTM –ú–û–î–ï–õ–ò V2\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     # 1. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\n",
        "#     print(\"üîß –≠—Ç–∞–ø 1: –†–∞—Å—á–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...\")\n",
        "#     enhanced_data = calculate_enhanced_technical_indicators(data)\n",
        "\n",
        "#     # 2. –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é\n",
        "#     print(\"üéØ –≠—Ç–∞–ø 2: –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π...\")\n",
        "#     final_data = create_target_variable(enhanced_data)\n",
        "\n",
        "#     # 3. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "#     print(\"üìä –≠—Ç–∞–ø 3: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...\")\n",
        "#     X, y, features, categories = prepare_enhanced_features_for_lstm(final_data)\n",
        "\n",
        "#     print(f\"\\nüìà –°–í–û–î–ö–ê –ü–û –î–ê–ù–ù–´–ú:\")\n",
        "#     print(f\"   üìä –û–±—Ä–∞–∑—Ü–æ–≤: {len(X):,}\")\n",
        "#     print(f\"   üî¢ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(features)} (–±—ã–ª–æ 19 ‚Üí –ø—Ä–∏—Ä–æ—Å—Ç +{len(features) - 19})\")\n",
        "#     print(f\"   ‚è±Ô∏è –ü–µ—Ä–∏–æ–¥: {final_data.index[0].strftime('%Y-%m-%d')} - {final_data.index[-1].strftime('%Y-%m-%d')}\")\n",
        "\n",
        "#     # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n",
        "#     print(f\"\\nüóÇÔ∏è –ü–†–ò–ó–ù–ê–ö–ò –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú:\")\n",
        "#     total_features = 0\n",
        "#     for category, cat_features in categories.items():\n",
        "#         if cat_features:\n",
        "#             count = len(cat_features)\n",
        "#             total_features += count\n",
        "#             print(f\"   üìÅ {category}: {count}\")\n",
        "#             # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –ø—Ä–∏–∑–Ω–∞–∫–∞ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
        "#             sample_features = ', '.join(cat_features[:3])\n",
        "#             if len(cat_features) > 3:\n",
        "#                 sample_features += f\", ... (+{len(cat_features)-3} –µ—â–µ)\"\n",
        "#             print(f\"      ‚îî‚îÄ {sample_features}\")\n",
        "\n",
        "#     print(f\"   üìä –í—Å–µ–≥–æ: {total_features} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "\n",
        "#     # 4. –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
        "#     print(f\"\\nü§ñ –≠—Ç–∞–ø 4: –°–æ–∑–¥–∞–Ω–∏–µ EnhancedForexLSTMModel...\")\n",
        "#     model = EnhancedForexLSTMModel()  # –ù–û–í–´–ô –ö–õ–ê–°–°!\n",
        "\n",
        "#     # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏\n",
        "#     print(f\"\\n‚öôÔ∏è –ü–ê–†–ê–ú–ï–¢–†–´ –ú–û–î–ï–õ–ò:\")\n",
        "#     print(f\"   üìè Sequence length: {model.sequence_length} –±–∞—Ä–æ–≤ ({model.sequence_length * 15} –º–∏–Ω—É—Ç)\")\n",
        "#     print(f\"   üß† LSTM –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {model.lstm_units}\")\n",
        "#     print(f\"   üëÅÔ∏è Attention heads: {model.attention_heads}\")\n",
        "#     print(f\"   üìö Batch size: {model.batch_size}\")\n",
        "#     print(f\"   üîÑ Max epochs: {model.epochs}\")\n",
        "#     print(f\"   üéØ Learning rate: {model.learning_rate}\")\n",
        "#     print(f\"   üìâ Dropout: {model.dropout_rate}\")\n",
        "#     print(f\"   ‚öñÔ∏è L2 regularization: {model.l2_reg}\")\n",
        "\n",
        "#     # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "#     data_result = model.prepare_data(X.values, y)\n",
        "#     if data_result[0] is None:\n",
        "#         print(\"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö!\")\n",
        "#         return None, None\n",
        "\n",
        "#     train_data, val_data, test_data = data_result\n",
        "\n",
        "#     print(f\"\\nüî• –≠—Ç–∞–ø 5: –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø...\")\n",
        "#     print(f\"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –û–±—É—á–µ–Ω–∏–µ –∑–∞–π–º–µ—Ç 20-30 –º–∏–Ω—É—Ç!\")\n",
        "#     print(f\"üìä –û–∂–∏–¥–∞–µ—Ç—Å—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞\")\n",
        "#     print(\"üîÑ –ú–æ–Ω–∏—Ç–æ—Ä–∏–º F1-macro score –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω—É—é –º–µ—Ç—Ä–∏–∫—É\")\n",
        "\n",
        "#     # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
        "#     history = model.train(train_data, val_data)\n",
        "\n",
        "#     # –û—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å\n",
        "#     print(f\"\\nüìä –≠—Ç–∞–ø 6: –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏...\")\n",
        "#     results = model.evaluate(test_data)\n",
        "\n",
        "#     # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å\n",
        "#     print(f\"\\nüíæ –≠—Ç–∞–ø 7: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\")\n",
        "#     model.save_model()\n",
        "\n",
        "#     # 8. –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é\n",
        "#     print(f\"\\nüìà –≠—Ç–∞–ø 8: –°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...\")\n",
        "#     create_enhanced_visualization(history, results, categories, features)\n",
        "\n",
        "#     # 9. –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞\n",
        "#     print_final_summary(results, len(features), model)\n",
        "\n",
        "#     return model, results, categories, features\n",
        "\n",
        "# def create_enhanced_visualization(history, results, categories, features):\n",
        "#     \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\"\"\"\n",
        "\n",
        "#     plt.figure(figsize=(24, 16))\n",
        "\n",
        "#     # 1. Training Loss\n",
        "#     plt.subplot(3, 4, 1)\n",
        "#     plt.plot(history.history['loss'], label='Train Loss', linewidth=2, color='red')\n",
        "#     plt.plot(history.history['val_loss'], label='Val Loss', linewidth=2, color='orange')\n",
        "#     plt.title('üìâ Training Loss', fontsize=12, fontweight='bold')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('Loss')\n",
        "#     plt.legend()\n",
        "#     plt.grid(True, alpha=0.3)\n",
        "\n",
        "#     # 2. Accuracy\n",
        "#     plt.subplot(3, 4, 2)\n",
        "#     plt.plot(history.history['accuracy'], label='Train Acc', linewidth=2, color='blue')\n",
        "#     plt.plot(history.history['val_accuracy'], label='Val Acc', linewidth=2, color='cyan')\n",
        "#     plt.title('üìà Accuracy', fontsize=12, fontweight='bold')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('Accuracy')\n",
        "#     plt.legend()\n",
        "#     plt.grid(True, alpha=0.3)\n",
        "\n",
        "#     # 3. F1 Score (–≥–ª–∞–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞)\n",
        "#     plt.subplot(3, 4, 3)\n",
        "#     plt.plot(history.history['f1_macro'], label='Train F1', linewidth=2, color='green')\n",
        "#     plt.plot(history.history['val_f1_macro'], label='Val F1', linewidth=2, color='lime')\n",
        "#     plt.title('üéØ F1-Macro Score', fontsize=12, fontweight='bold')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('F1 Score')\n",
        "#     plt.legend()\n",
        "#     plt.grid(True, alpha=0.3)\n",
        "\n",
        "#     # 4. Learning Rate\n",
        "#     plt.subplot(3, 4, 4)\n",
        "#     if 'lr' in history.history:\n",
        "#         plt.plot(history.history['lr'], linewidth=2, color='purple')\n",
        "#         plt.title('üìä Learning Rate', fontsize=12, fontweight='bold')\n",
        "#         plt.xlabel('Epoch')\n",
        "#         plt.ylabel('LR')\n",
        "#         plt.yscale('log')\n",
        "#         plt.grid(True, alpha=0.3)\n",
        "\n",
        "#     # 5. Confusion Matrix\n",
        "#     plt.subplot(3, 4, 5)\n",
        "#     cm = results['confusion_matrix']\n",
        "#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "#                 xticklabels=['SELL', 'HOLD', 'BUY'],\n",
        "#                 yticklabels=['SELL', 'HOLD', 'BUY'],\n",
        "#                 cbar_kws={'label': 'Count'})\n",
        "#     plt.title('üéØ Confusion Matrix', fontsize=12, fontweight='bold')\n",
        "\n",
        "#     # 6. Class-wise Precision\n",
        "#     plt.subplot(3, 4, 6)\n",
        "#     class_names = ['SELL', 'HOLD', 'BUY']\n",
        "#     precision_scores = [\n",
        "#         results['classification_report']['SELL']['precision'],\n",
        "#         results['classification_report']['HOLD']['precision'],\n",
        "#         results['classification_report']['BUY']['precision']\n",
        "#     ]\n",
        "#     bars = plt.bar(class_names, precision_scores, color=['red', 'gray', 'green'], alpha=0.7)\n",
        "#     plt.title('üìä Precision by Class', fontsize=12, fontweight='bold')\n",
        "#     plt.ylabel('Precision')\n",
        "#     plt.ylim(0, 1)\n",
        "#     # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã\n",
        "#     for bar, score in zip(bars, precision_scores):\n",
        "#         plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "#                 f'{score:.3f}', ha='center', va='bottom')\n",
        "#     plt.grid(True, alpha=0.3)\n",
        "\n",
        "#     # 7. Class-wise Recall\n",
        "#     plt.subplot(3, 4, 7)\n",
        "#     recall_scores = [\n",
        "#         results['classification_report']['SELL']['recall'],\n",
        "#         results['classification_report']['HOLD']['recall'],\n",
        "#         results['classification_report']['BUY']['recall']\n",
        "#     ]\n",
        "#     bars = plt.bar(class_names, recall_scores, color=['red', 'gray', 'green'], alpha=0.7)\n",
        "#     plt.title('üìä Recall by Class', fontsize=12, fontweight='bold')\n",
        "#     plt.ylabel('Recall')\n",
        "#     plt.ylim(0, 1)\n",
        "#     for bar, score in zip(bars, recall_scores):\n",
        "#         plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "#                 f'{score:.3f}', ha='center', va='bottom')\n",
        "#     plt.grid(True, alpha=0.3)\n",
        "\n",
        "#     # 8. Feature Categories\n",
        "#     plt.subplot(3, 4, 8)\n",
        "#     category_counts = {k: len(v) for k, v in categories.items() if v}\n",
        "#     categories_short = [k[:8] for k in category_counts.keys()]  # –°–æ–∫—Ä–∞—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è\n",
        "#     plt.pie(category_counts.values(), labels=categories_short, autopct='%1.0f%%', startangle=90)\n",
        "#     plt.title('üóÇÔ∏è Features by Category', fontsize=12, fontweight='bold')\n",
        "\n",
        "#     # 9. Predictions Distribution\n",
        "#     plt.subplot(3, 4, 9)\n",
        "#     pred_classes = results['predictions_class']\n",
        "#     unique, counts = np.unique(pred_classes, return_counts=True)\n",
        "#     pred_counts = [counts[unique == i][0] if i in unique else 0 for i in range(3)]\n",
        "#     plt.pie(pred_counts, labels=class_names, autopct='%1.1f%%',\n",
        "#             colors=['red', 'gray', 'green'], startangle=90)\n",
        "#     plt.title('üéØ Predictions Distribution', fontsize=12, fontweight='bold')\n",
        "\n",
        "#     # 10. Model Confidence\n",
        "#     plt.subplot(3, 4, 10)\n",
        "#     confidence = results['confidence']\n",
        "#     plt.hist(confidence, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "#     plt.axvline(confidence.mean(), color='red', linestyle='--',\n",
        "#                 label=f'Mean: {confidence.mean():.3f}')\n",
        "#     plt.title('üìä Model Confidence', fontsize=12, fontweight='bold')\n",
        "#     plt.xlabel('Max Probability')\n",
        "#     plt.ylabel('Count')\n",
        "#     plt.legend()\n",
        "#     plt.grid(True, alpha=0.3)\n",
        "\n",
        "#     # 11. Class Performance Comparison\n",
        "#     plt.subplot(3, 4, 11)\n",
        "#     x = np.arange(len(class_names))\n",
        "#     width = 0.35\n",
        "\n",
        "#     plt.bar(x - width/2, precision_scores, width, label='Precision', alpha=0.7, color='blue')\n",
        "#     plt.bar(x + width/2, recall_scores, width, label='Recall', alpha=0.7, color='orange')\n",
        "\n",
        "#     plt.xlabel('Classes')\n",
        "#     plt.ylabel('Score')\n",
        "#     plt.title('‚öñÔ∏è Precision vs Recall', fontsize=12, fontweight='bold')\n",
        "#     plt.xticks(x, class_names)\n",
        "#     plt.legend()\n",
        "#     plt.grid(True, alpha=0.3)\n",
        "\n",
        "#     # 12. Training Progress Summary\n",
        "#     plt.subplot(3, 4, 12)\n",
        "#     epochs = range(1, len(history.history['loss']) + 1)\n",
        "#     plt.plot(epochs, history.history['accuracy'], 'b-', label='Train Acc', alpha=0.7)\n",
        "#     plt.plot(epochs, history.history['val_accuracy'], 'r-', label='Val Acc', alpha=0.7)\n",
        "#     plt.plot(epochs, history.history['f1_macro'], 'g-', label='Train F1', alpha=0.7)\n",
        "#     plt.plot(epochs, history.history['val_f1_macro'], 'm-', label='Val F1', alpha=0.7)\n",
        "\n",
        "#     plt.title('üìà Training Summary', fontsize=12, fontweight='bold')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('Score')\n",
        "#     plt.legend()\n",
        "#     plt.grid(True, alpha=0.3)\n",
        "\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# def print_final_summary(results, num_features, model):\n",
        "#     \"\"\"–ü–µ—á–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å–≤–æ–¥–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\"\"\"\n",
        "\n",
        "#     print(\"\\n\" + \"=\"*70)\n",
        "#     print(\"üéØ –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ ENHANCED LSTM MODEL\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
        "#     print(f\"üìä –û–°–ù–û–í–ù–´–ï –ú–ï–¢–†–ò–ö–ò:\")\n",
        "#     print(f\"   üéØ Accuracy: {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)\")\n",
        "#     print(f\"   üèÜ F1-Macro: {results['f1_macro']:.4f} ({results['f1_macro']*100:.2f}%)\")\n",
        "#     print(f\"   üìâ Loss: {results['loss']:.4f}\")\n",
        "\n",
        "#     # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä—Å–∏–µ–π\n",
        "#     print(f\"\\nüìà –°–†–ê–í–ù–ï–ù–ò–ï –° –ü–†–ï–î–´–î–£–©–ï–ô –í–ï–†–°–ò–ï–ô:\")\n",
        "#     print(f\"   üî¢ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {num_features} (–±—ã–ª–æ 19, –ø—Ä–∏—Ä–æ—Å—Ç +{num_features-19})\")\n",
        "#     print(f\"   üìè Sequence length: {model.sequence_length} (–±—ã–ª–æ 64)\")\n",
        "#     print(f\"   üß† LSTM —Å–ª–æ–∏: {len(model.lstm_units)} (–±—ã–ª–æ 2)\")\n",
        "#     print(f\"   üëÅÔ∏è Attention heads: {model.attention_heads} (–±—ã–ª–æ 4)\")\n",
        "\n",
        "#     # –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º\n",
        "#     print(f\"\\nüìã –î–ï–¢–ê–õ–¨–ù–´–ï –ú–ï–¢–†–ò–ö–ò –ü–û –ö–õ–ê–°–°–ê–ú:\")\n",
        "#     class_names = ['SELL', 'HOLD', 'BUY']\n",
        "#     for class_name in class_names:\n",
        "#         if class_name in results['classification_report']:\n",
        "#             report = results['classification_report'][class_name]\n",
        "#             print(f\"   {class_name}:\")\n",
        "#             print(f\"     Precision: {report['precision']:.4f}\")\n",
        "#             print(f\"     Recall: {report['recall']:.4f}\")\n",
        "#             print(f\"     F1-score: {report['f1-score']:.4f}\")\n",
        "#             print(f\"     Support: {report['support']}\")\n",
        "\n",
        "#     # –ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏\n",
        "#     confidence = results['confidence']\n",
        "#     print(f\"\\nüéØ –ê–ù–ê–õ–ò–ó –£–í–ï–†–ï–ù–ù–û–°–¢–ò –ú–û–î–ï–õ–ò:\")\n",
        "#     print(f\"   üìä –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence.mean():.3f}\")\n",
        "#     print(f\"   üìä –ú–µ–¥–∏–∞–Ω–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {np.median(confidence):.3f}\")\n",
        "#     print(f\"   ‚úÖ –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (>0.8): {(confidence > 0.8).sum()}/{len(confidence)} ({(confidence > 0.8).sum()/len(confidence)*100:.1f}%)\")\n",
        "#     print(f\"   ‚ö†Ô∏è –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (<0.5): {(confidence < 0.5).sum()}/{len(confidence)} ({(confidence < 0.5).sum()/len(confidence)*100:.1f}%)\")\n",
        "\n",
        "#     # –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
        "#     print(f\"\\nüí° –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:\")\n",
        "#     if results['accuracy'] > 0.45:\n",
        "#         print(f\"   ‚úÖ –û—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç! –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é\")\n",
        "#     elif results['accuracy'] > 0.40:\n",
        "#         print(f\"   üìà –•–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç! –ú–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å –µ—â–µ –Ω–µ–º–Ω–æ–≥–æ\")\n",
        "#     else:\n",
        "#         print(f\"   ‚ö†Ô∏è –†–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∏–∂–µ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è:\")\n",
        "#         print(f\"     ‚Ä¢ –î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö\")\n",
        "#         print(f\"     ‚Ä¢ –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã\")\n",
        "#         print(f\"     ‚Ä¢ –ü—Ä–æ–≤–µ—Å—Ç–∏ feature engineering\")\n",
        "\n",
        "#     print(f\"\\nüíæ –°–û–•–†–ê–ù–ï–ù–ò–ï:\")\n",
        "#     print(f\"   üìÅ –ú–æ–¥–µ–ª—å: models/enhanced_forex_model.h5\")\n",
        "#     print(f\"   üìÅ Scaler: models/scaler.pkl\")\n",
        "#     print(f\"   üìÅ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: models/best_enhanced_model.h5\")\n",
        "\n",
        "#     print(\"=\"*70)\n",
        "#     print(\"‚úÖ –û–ë–£–ß–ï–ù–ò–ï ENHANCED –ú–û–î–ï–õ–ò –ó–ê–í–ï–†–®–ï–ù–û!\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "# # –û–°–ù–û–í–ù–û–ô –ó–ê–ü–£–°–ö\n",
        "# if 'forex_data' in locals():\n",
        "#     print(\"üî• –í–°–ï –ì–û–¢–û–í–û! –ó–ê–ü–£–°–ö–ê–ï–ú ENHANCED –û–ë–£–ß–ï–ù–ò–ï V2...\")\n",
        "#     print(\"‚è±Ô∏è –û–∂–∏–¥–∞–π—Ç–µ 25-35 –º–∏–Ω—É—Ç (—Å–ª–æ–∂–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)\")\n",
        "#     print(\"üéØ –û–∂–∏–¥–∞–µ—Ç—Å—è accuracy 50-60% –∏ F1-score 0.45-0.55\")\n",
        "#     print(\"üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–º F1-macro –∫–∞–∫ –≥–ª–∞–≤–Ω—É—é –º–µ—Ç—Ä–∏–∫—É\")\n",
        "#     print(\"\")\n",
        "\n",
        "#     enhanced_model_v2, enhanced_results_v2, feature_categories_v2, features_v2 = train_enhanced_forex_model_v2(forex_data)\n",
        "# else:\n",
        "#     print(\"‚ö†Ô∏è Forex –¥–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –í—ã–ø–æ–ª–Ω–∏—Ç–µ —è—á–µ–π–∫—É –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö!\")\n",
        "\n",
        "# print(\"\\nüöÄ –ß–¢–û –ù–û–í–û–ì–û –í V2:\")\n",
        "# print(\"‚úÖ EnhancedForexLSTMModel –∫–ª–∞—Å—Å\")\n",
        "# print(\"‚úÖ F1-macro –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞\")\n",
        "# print(\"‚úÖ Precision/Recall –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞\")\n",
        "# print(\"‚úÖ –ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏\")\n",
        "# print(\"‚úÖ –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ scaler –∏ –º–æ–¥–µ–ª–∏\")\n",
        "# print(\"‚úÖ –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è (12 –≥—Ä–∞—Ñ–∏–∫–æ–≤)\")\n",
        "# print(\"‚úÖ –î–µ—Ç–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\")\n",
        "# print(\"‚úÖ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\")"
      ],
      "metadata": {
        "id": "eVrFs9_6lzjF"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost shap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIl7TvuX4Y8I",
        "outputId": "cb21c5f5-2962-40bb-bf35-54a32ae8e027"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.8)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.47.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from shap) (1.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (24.2)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from shap) (4.14.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –ó–ê–ì–†–£–ó–ö–ê –°–û–•–†–ê–ù–ï–ù–ù–û–ô LSTM –ú–û–î–ï–õ–ò –î–õ–Ø –ê–ù–°–ê–ú–ë–õ–Ø\n",
        "# ==================================================================================\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "def load_saved_model_and_prepare_ensemble():\n",
        "    \"\"\"\n",
        "    –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π LSTM –º–æ–¥–µ–ª–∏ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è\n",
        "    \"\"\"\n",
        "    print(\"üîÑ –ó–ê–ì–†–£–ñ–ê–ï–ú –°–û–•–†–ê–ù–ï–ù–ù–£–Æ –ú–û–î–ï–õ–¨...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤\n",
        "    model_path = 'models/best_enhanced_model.h5'\n",
        "    scaler_path = 'models/scaler.pkl'\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}\")\n",
        "        return None\n",
        "\n",
        "    if not os.path.exists(scaler_path):\n",
        "        print(f\"‚ùå Scaler –Ω–µ –Ω–∞–π–¥–µ–Ω: {scaler_path}\")\n",
        "        return None\n",
        "\n",
        "    # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º LSTM –º–æ–¥–µ–ª—å\n",
        "    print(\"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ LSTM –º–æ–¥–µ–ª–∏...\")\n",
        "    try:\n",
        "        # –ö–∞—Å—Ç–æ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω—É–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏\n",
        "        def f1_macro(y_true, y_pred):\n",
        "            def f1_score_class(class_id):\n",
        "                y_true_class = tf.cast(tf.equal(tf.argmax(y_true, axis=1), class_id), tf.float32)\n",
        "                y_pred_class = tf.cast(tf.equal(tf.argmax(y_pred, axis=1), class_id), tf.float32)\n",
        "\n",
        "                tp = tf.reduce_sum(y_true_class * y_pred_class)\n",
        "                fp = tf.reduce_sum((1 - y_true_class) * y_pred_class)\n",
        "                fn = tf.reduce_sum(y_true_class * (1 - y_pred_class))\n",
        "\n",
        "                precision = tp / (tp + fp + tf.keras.backend.epsilon())\n",
        "                recall = tp / (tp + fn + tf.keras.backend.epsilon())\n",
        "\n",
        "                return 2 * precision * recall / (precision + recall + tf.keras.backend.epsilon())\n",
        "\n",
        "            f1_0 = f1_score_class(0)\n",
        "            f1_1 = f1_score_class(1)\n",
        "            f1_2 = f1_score_class(2)\n",
        "\n",
        "            return (f1_0 + f1_1 + f1_2) / 3\n",
        "\n",
        "        def precision_class(class_id):\n",
        "            def precision(y_true, y_pred):\n",
        "                y_true_class = tf.cast(tf.equal(tf.argmax(y_true, axis=1), class_id), tf.float32)\n",
        "                y_pred_class = tf.cast(tf.equal(tf.argmax(y_pred, axis=1), class_id), tf.float32)\n",
        "\n",
        "                tp = tf.reduce_sum(y_true_class * y_pred_class)\n",
        "                fp = tf.reduce_sum((1 - y_true_class) * y_pred_class)\n",
        "\n",
        "                return tp / (tp + fp + tf.keras.backend.epsilon())\n",
        "            return precision\n",
        "\n",
        "        def recall_class(class_id):\n",
        "            def recall(y_true, y_pred):\n",
        "                y_true_class = tf.cast(tf.equal(tf.argmax(y_true, axis=1), class_id), tf.float32)\n",
        "                y_pred_class = tf.cast(tf.equal(tf.argmax(y_pred, axis=1), class_id), tf.float32)\n",
        "\n",
        "                tp = tf.reduce_sum(y_true_class * y_pred_class)\n",
        "                fn = tf.reduce_sum(y_true_class * (1 - y_pred_class))\n",
        "\n",
        "                return tp / (tp + fn + tf.keras.backend.epsilon())\n",
        "            return recall\n",
        "\n",
        "        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏\n",
        "        custom_objects = {\n",
        "            'f1_macro': f1_macro,\n",
        "            'precision': precision_class(0),\n",
        "            'precision_1': precision_class(1),\n",
        "            'precision_2': precision_class(2),\n",
        "            'recall': recall_class(0),\n",
        "            'recall_1': recall_class(1),\n",
        "            'recall_2': recall_class(2)\n",
        "        }\n",
        "\n",
        "        lstm_model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)\n",
        "        print(\"‚úÖ LSTM –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}\")\n",
        "        return None\n",
        "\n",
        "    # 3. –ó–∞–≥—Ä—É–∂–∞–µ–º scaler\n",
        "    print(\"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ scaler...\")\n",
        "    try:\n",
        "        with open(scaler_path, 'rb') as f:\n",
        "            scaler = pickle.load(f)\n",
        "        print(\"‚úÖ Scaler –∑–∞–≥—Ä—É–∂–µ–Ω!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ scaler: {e}\")\n",
        "        return None\n",
        "\n",
        "    # 4. –í–æ—Å—Å–æ–∑–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã)\n",
        "    if 'forex_data' not in globals():\n",
        "        print(\"‚ùå forex_data –Ω–µ –Ω–∞–π–¥–µ–Ω! –ó–∞–ø—É—Å—Ç–∏—Ç–µ —è—á–µ–π–∫–∏ 1-5!\")\n",
        "        return None\n",
        "\n",
        "    print(\"üîß –í–æ—Å—Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...\")\n",
        "\n",
        "    # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ\n",
        "    enhanced_data = calculate_enhanced_technical_indicators(forex_data)\n",
        "    final_data = create_target_variable(enhanced_data)\n",
        "    X, y, features, categories = prepare_enhanced_features_for_lstm(final_data)\n",
        "\n",
        "    print(f\"‚úÖ –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã: X{X.shape}, y{y.shape}\")\n",
        "    print(f\"üìä –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(features)}\")\n",
        "\n",
        "    # 5. –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –º–æ–¥–µ–ª–∏\n",
        "    class LoadedEnhancedModel:\n",
        "        def __init__(self, model, scaler):\n",
        "            self.model = model\n",
        "            self.scaler = scaler\n",
        "            self.sequence_length = 96  # –ò–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫\n",
        "\n",
        "        def predict(self, X_sequences):\n",
        "            return self.model.predict(X_sequences, verbose=0)\n",
        "\n",
        "        def prepare_data(self, X, y):\n",
        "            \"\"\"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏\"\"\"\n",
        "            print(\"üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
        "\n",
        "            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "            X_scaled = self.scaler.transform(X)\n",
        "\n",
        "            # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π\n",
        "            X_sequences = []\n",
        "            y_sequences = []\n",
        "\n",
        "            for i in range(self.sequence_length, len(X_scaled)):\n",
        "                X_sequences.append(X_scaled[i-self.sequence_length:i])\n",
        "                y_sequences.append(y[i])\n",
        "\n",
        "            X_sequences = np.array(X_sequences, dtype=np.float32)\n",
        "            y_sequences = np.array(y_sequences)\n",
        "\n",
        "            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö\n",
        "            total_samples = len(X_sequences)\n",
        "            test_size = int(total_samples * 0.15)\n",
        "            val_size = int(total_samples * 0.15)\n",
        "            train_size = total_samples - test_size - val_size\n",
        "\n",
        "            X_train = X_sequences[:train_size]\n",
        "            y_train = y_sequences[:train_size]\n",
        "            X_val = X_sequences[train_size:train_size + val_size]\n",
        "            y_val = y_sequences[train_size:train_size + val_size]\n",
        "            X_test = X_sequences[train_size + val_size:]\n",
        "            y_test = y_sequences[train_size + val_size:]\n",
        "\n",
        "            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
        "            def to_categorical_custom(y_data):\n",
        "                y_cat = y_data + 1\n",
        "                return tf.keras.utils.to_categorical(y_cat, num_classes=3)\n",
        "\n",
        "            y_train_cat = to_categorical_custom(y_train)\n",
        "            y_val_cat = to_categorical_custom(y_val)\n",
        "            y_test_cat = to_categorical_custom(y_test)\n",
        "\n",
        "            print(f\"üìä Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
        "\n",
        "            return (X_train, y_train_cat), (X_val, y_val_cat), (X_test, y_test_cat)\n",
        "\n",
        "    # 6. –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
        "    enhanced_model_v2 = LoadedEnhancedModel(lstm_model, scaler)\n",
        "\n",
        "    print(\"\\n‚úÖ –í–°–ï –ì–û–¢–û–í–û!\")\n",
        "    print(\"üìä –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω—ã:\")\n",
        "    print(\"   ‚Ä¢ enhanced_model_v2 (–∑–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è LSTM)\")\n",
        "    print(\"   ‚Ä¢ X, y (–¥–∞–Ω–Ω—ã–µ)\")\n",
        "    print(\"   ‚Ä¢ features_v2 (—Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)\")\n",
        "    print(\"   ‚Ä¢ categories (–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)\")\n",
        "\n",
        "    return enhanced_model_v2, X, y, features, categories\n",
        "\n",
        "# –í–´–ü–û–õ–ù–Ø–ï–ú –ó–ê–ì–†–£–ó–ö–£\n",
        "print(\"üöÄ –ó–ê–ì–†–£–ñ–ê–ï–ú –°–û–•–†–ê–ù–ï–ù–ù–£–Æ –ú–û–î–ï–õ–¨...\")\n",
        "result = load_saved_model_and_prepare_ensemble()\n",
        "\n",
        "if result is not None:\n",
        "    enhanced_model_v2, X, y, features_v2, categories = result\n",
        "\n",
        "    print(\"\\nüî• –ì–û–¢–û–í–û –ö –ó–ê–ü–£–°–ö–£ –ê–ù–°–ê–ú–ë–õ–Ø!\")\n",
        "    print(\"üí° –¢–µ–ø–µ—Ä—å –º–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å:\")\n",
        "    print(\"   ensemble_model, ensemble_results, feature_importance = train_lstm_catboost_ensemble()\")\n",
        "else:\n",
        "    print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJKjcRGYzE82",
        "outputId": "76db5d35-2178-4fcf-becf-c78084dc0777"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ –ó–ê–ì–†–£–ñ–ê–ï–ú –°–û–•–†–ê–ù–ï–ù–ù–£–Æ –ú–û–î–ï–õ–¨...\n",
            "üîÑ –ó–ê–ì–†–£–ñ–ê–ï–ú –°–û–•–†–ê–ù–ï–ù–ù–£–Æ –ú–û–î–ï–õ–¨...\n",
            "==================================================\n",
            "üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ LSTM –º–æ–¥–µ–ª–∏...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ LSTM –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!\n",
            "üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ scaler...\n",
            "‚úÖ Scaler –∑–∞–≥—Ä—É–∂–µ–Ω!\n",
            "üîß –í–æ—Å—Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...\n",
            "üîß –†–∞—Å—á–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...\n",
            "‚úÖ –†–∞—Å—Å—á–∏—Ç–∞–Ω–æ 66 –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤\n",
            "üìä –î–æ–±–∞–≤–ª–µ–Ω–æ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞: ADX, Williams %R, CCI, Stochastic\n",
            "üî• –ù–æ–≤—ã–µ —Ñ–∏—á–∏: –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è, –∫–ª–∞—Å—Ç–µ—Ä—ã, —Å–µ—Å—Å–∏–∏\n",
            "üéØ –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (–ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 60 –º–∏–Ω—É—Ç)...\n",
            "üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:\n",
            "   SELL: 1,856 (32.8%)\n",
            "   HOLD: 1,710 (30.2%)\n",
            "   BUY: 2,087 (36.9%)\n",
            "üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è LSTM...\n",
            "‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: 63 (–±—ã–ª–æ 19)\n",
            "üìä –ß–∏—Å—Ç—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: 5,587\n",
            "\n",
            "üìã –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:\n",
            "   –û—Å–Ω–æ–≤–Ω—ã–µ: 6 (Close, High, Low...)\n",
            "   –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ: 12 (SMA_9, SMA_21, EMA_12...)\n",
            "   –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ: 13 (ADX, DI_Plus, DI_Minus...)\n",
            "   –í—Ä–µ–º–µ–Ω–Ω—ã–µ: 5 (Hour, DayOfWeek, IsLondonSession...)\n",
            "   –°–∏–≥–Ω–∞–ª—ã: 11 (MA_Cross, EMA_Cross, MACD_Cross...)\n",
            "   –õ–∞–≥–∏: 10 (Returns_lag_1, Returns_lag_2, Returns_lag_3...)\n",
            "   –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è: 4 (RSI_MACD_Interaction, BB_RSI_Interaction, ADX_RSI_Strength...)\n",
            "   –ö–ª–∞—Å—Ç–µ—Ä—ã: 2 (Bullish_Cluster, Bearish_Cluster)\n",
            "\n",
            "üìä –ò—Ç–æ–≥–æ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:\n",
            "   SELL: 1,837 (32.9%)\n",
            "   HOLD: 1,681 (30.1%)\n",
            "   BUY: 2,069 (37.0%)\n",
            "‚úÖ –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã: X(5587, 63), y(5587,)\n",
            "üìä –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: 63\n",
            "\n",
            "‚úÖ –í–°–ï –ì–û–¢–û–í–û!\n",
            "üìä –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω—ã:\n",
            "   ‚Ä¢ enhanced_model_v2 (–∑–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è LSTM)\n",
            "   ‚Ä¢ X, y (–¥–∞–Ω–Ω—ã–µ)\n",
            "   ‚Ä¢ features_v2 (—Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)\n",
            "   ‚Ä¢ categories (–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)\n",
            "\n",
            "üî• –ì–û–¢–û–í–û –ö –ó–ê–ü–£–°–ö–£ –ê–ù–°–ê–ú–ë–õ–Ø!\n",
            "üí° –¢–µ–ø–µ—Ä—å –º–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å:\n",
            "   ensemble_model, ensemble_results, feature_importance = train_lstm_catboost_ensemble()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –ü–ï–†–ï–ú–ï–©–ê–ï–ú –§–ê–ô–õ –ò –°–û–ó–î–ê–ï–ú –ù–£–ñ–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "print(\"üîß –ò–°–ü–†–ê–í–õ–Ø–ï–ú –ü–£–¢–ò –§–ê–ô–õ–û–í...\")\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É models\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏\n",
        "if os.path.exists('best_model.h5'):\n",
        "    shutil.move('best_model.h5', 'models/best_enhanced_model.h5')\n",
        "    print(\"‚úÖ –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–º–µ—â–µ–Ω–∞ –≤ models/best_enhanced_model.h5\")\n",
        "else:\n",
        "    print(\"‚ùå best_model.h5 –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫–æ—Ä–Ω–µ\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å scaler –≤ –ø–∞–º—è—Ç–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ–≥–æ\n",
        "import pickle\n",
        "\n",
        "# –ò—â–µ–º scaler –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö\n",
        "scaler_found = False\n",
        "for var_name in ['enhanced_model_v2', 'trained_model']:\n",
        "    if var_name in globals():\n",
        "        var = globals()[var_name]\n",
        "        if hasattr(var, 'scaler'):\n",
        "            print(f\"‚úÖ –ù–∞–π–¥–µ–Ω scaler –≤ {var_name}\")\n",
        "            with open('models/scaler.pkl', 'wb') as f:\n",
        "                pickle.dump(var.scaler, f)\n",
        "            print(\"‚úÖ Scaler —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ models/scaler.pkl\")\n",
        "            scaler_found = True\n",
        "            break\n",
        "\n",
        "if not scaler_found:\n",
        "    print(\"‚ö†Ô∏è Scaler –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π...\")\n",
        "    # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º scaler\n",
        "    from sklearn.preprocessing import RobustScaler\n",
        "    enhanced_data = calculate_enhanced_technical_indicators(forex_data)\n",
        "    final_data = create_target_variable(enhanced_data)\n",
        "    X, y, features_v2, categories = prepare_enhanced_features_for_lstm(final_data)\n",
        "\n",
        "    scaler = RobustScaler()\n",
        "    scaler.fit(X.values)\n",
        "\n",
        "    with open('models/scaler.pkl', 'wb') as f:\n",
        "        pickle.dump(scaler, f)\n",
        "    print(\"‚úÖ –ù–æ–≤—ã–π scaler —Å–æ–∑–¥–∞–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω!\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
        "model_exists = os.path.exists('models/best_enhanced_model.h5')\n",
        "scaler_exists = os.path.exists('models/scaler.pkl')\n",
        "\n",
        "print(f\"\\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢:\")\n",
        "print(f\"üìÅ –ú–æ–¥–µ–ª—å: {model_exists}\")\n",
        "print(f\"üìÅ Scaler: {scaler_exists}\")\n",
        "\n",
        "if model_exists and scaler_exists:\n",
        "    print(\"üî• –í–°–ï –ì–û–¢–û–í–û! –¢–µ–ø–µ—Ä—å –∑–∞–ø—É—Å—Ç–∏—Ç–µ:\")\n",
        "    print(\"result = load_saved_model_and_prepare_ensemble()\")\n",
        "    print(\"enhanced_model_v2, X, y, features_v2, categories = result\")\n",
        "else:\n",
        "    print(\"‚ùå –ß—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klWZ514jRCQY",
        "outputId": "59143eec-f488-4c8c-d5f6-ac01b2e694c7"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß –ò–°–ü–†–ê–í–õ–Ø–ï–ú –ü–£–¢–ò –§–ê–ô–õ–û–í...\n",
            "‚ùå best_model.h5 –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫–æ—Ä–Ω–µ\n",
            "‚úÖ –ù–∞–π–¥–µ–Ω scaler –≤ enhanced_model_v2\n",
            "‚úÖ Scaler —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ models/scaler.pkl\n",
            "\n",
            "üìä –†–ï–ó–£–õ–¨–¢–ê–¢:\n",
            "üìÅ –ú–æ–¥–µ–ª—å: True\n",
            "üìÅ Scaler: True\n",
            "üî• –í–°–ï –ì–û–¢–û–í–û! –¢–µ–ø–µ—Ä—å –∑–∞–ø—É—Å—Ç–∏—Ç–µ:\n",
            "result = load_saved_model_and_prepare_ensemble()\n",
            "enhanced_model_v2, X, y, features_v2, categories = result\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –¢–û–õ–¨–ö–û CATBOOST –ú–û–î–ï–õ–¨ (–ë–ï–ó –ê–ù–°–ê–ú–ë–õ–Ø –° LSTM)\n",
        "# ==================================================================================\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "import shap\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "def train_catboost_standalone():\n",
        "    \"\"\"\n",
        "    –û–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ CatBoost –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (63 –ø—Ä–∏–∑–Ω–∞–∫–∞)\n",
        "    \"\"\"\n",
        "    print(\"üöÄ –û–ë–£–ß–ï–ù–ò–ï CATBOOST –ú–û–î–ï–õ–ò (STANDALONE)\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"üí° –û–±—Ö–æ–¥–∏–º –ø—Ä–æ–±–ª–µ–º—É —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å LSTM\")\n",
        "\n",
        "    # 1. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "    print(\"üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
        "    enhanced_data = calculate_enhanced_technical_indicators(forex_data)\n",
        "    final_data = create_target_variable(enhanced_data)\n",
        "    X, y, features, categories = prepare_enhanced_features_for_lstm(final_data)\n",
        "\n",
        "    print(f\"üìä –î–∞–Ω–Ω—ã–µ: X{X.shape}, y{y.shape}\")\n",
        "    print(f\"üî¢ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(features)}\")\n",
        "\n",
        "    # 2. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
        "    from sklearn.preprocessing import RobustScaler\n",
        "    scaler = RobustScaler()\n",
        "    X_scaled = scaler.fit_transform(X.values)\n",
        "\n",
        "    # 3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (–∫–∞–∫ —É LSTM)\n",
        "    total_samples = len(X_scaled)\n",
        "    test_size = int(total_samples * 0.15)\n",
        "    val_size = int(total_samples * 0.15)\n",
        "    train_size = total_samples - test_size - val_size\n",
        "\n",
        "    X_train = X_scaled[:train_size]\n",
        "    y_train = y[:train_size]\n",
        "    X_val = X_scaled[train_size:train_size + val_size]\n",
        "    y_val = y[train_size:train_size + val_size]\n",
        "    X_test = X_scaled[train_size + val_size:]\n",
        "    y_test = y[train_size + val_size:]\n",
        "\n",
        "    print(f\"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "    print(f\"   Train: {len(X_train)}\")\n",
        "    print(f\"   Val: {len(X_val)}\")\n",
        "    print(f\"   Test: {len(X_test)}\")\n",
        "\n",
        "    # 4. –û–±—É—á–µ–Ω–∏–µ CatBoost\n",
        "    print(\"\\nüöÄ –û–±—É—á–µ–Ω–∏–µ CatBoost –º–æ–¥–µ–ª–∏...\")\n",
        "\n",
        "    catboost_model = CatBoostClassifier(\n",
        "        iterations=1000,\n",
        "        learning_rate=0.1,\n",
        "        depth=6,\n",
        "        l2_leaf_reg=3,\n",
        "        random_seed=42,\n",
        "        verbose=100,\n",
        "        early_stopping_rounds=50,\n",
        "        eval_metric='MultiClass',\n",
        "        custom_metric=['F1'],\n",
        "        auto_class_weights='Balanced',\n",
        "        bootstrap_type='Bernoulli',\n",
        "        subsample=0.8,\n",
        "        colsample_bylevel=0.8,\n",
        "        use_best_model=True\n",
        "    )\n",
        "\n",
        "    # –û–±—É—á–µ–Ω–∏–µ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π\n",
        "    catboost_model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=(X_val, y_val),\n",
        "        plot=False\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ CatBoost –æ–±—É—á–µ–Ω!\")\n",
        "\n",
        "    # 5. –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "    feature_importance = catboost_model.get_feature_importance()\n",
        "    feature_importance_df = pd.DataFrame({\n",
        "        'feature': features,\n",
        "        'importance': feature_importance\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    print(f\"\\nüìä –¢–û–ü-15 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\")\n",
        "    for i, row in feature_importance_df.head(15).iterrows():\n",
        "        print(f\"   {row['feature']}: {row['importance']:.1f}\")\n",
        "\n",
        "    # 6. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "    print(\"\\nü§ñ –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...\")\n",
        "    y_pred_proba = catboost_model.predict_proba(X_test)\n",
        "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "    # 7. –û—Ü–µ–Ω–∫–∞\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "    print(f\"\\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ CATBOOST:\")\n",
        "    print(f\"   Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "    print(f\"   F1-macro: {f1_macro:.4f}\")\n",
        "\n",
        "    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç\n",
        "    class_names = ['SELL', 'HOLD', 'BUY']\n",
        "    print(f\"\\nüìã –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=class_names))\n",
        "\n",
        "    # 8. SHAP –∞–Ω–∞–ª–∏–∑\n",
        "    print(\"\\nüîç SHAP –∞–Ω–∞–ª–∏–∑...\")\n",
        "    try:\n",
        "        # –ë–µ—Ä–µ–º –Ω–µ–±–æ–ª—å—à—É—é –≤—ã–±–æ—Ä–∫—É –¥–ª—è SHAP\n",
        "        sample_size = min(100, len(X_test))\n",
        "        X_sample = X_test[:sample_size]\n",
        "\n",
        "        explainer = shap.TreeExplainer(catboost_model)\n",
        "        shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "        print(f\"‚úÖ SHAP –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è {sample_size} –æ–±—Ä–∞–∑—Ü–æ–≤\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è SHAP –∞–Ω–∞–ª–∏–∑ –Ω–µ —É–¥–∞–ª—Å—è: {e}\")\n",
        "        explainer, shap_values = None, None\n",
        "\n",
        "    # 9. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "    print(\"\\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\")\n",
        "    catboost_model.save_model('models/catboost_standalone.cbm')\n",
        "    joblib.dump(scaler, 'models/catboost_scaler.pkl')\n",
        "    joblib.dump(feature_importance_df, 'models/feature_importance.pkl')\n",
        "\n",
        "    # 10. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "    create_catboost_visualization(\n",
        "        catboost_model, y_test, y_pred, y_pred_proba,\n",
        "        feature_importance_df, categories, features\n",
        "    )\n",
        "\n",
        "    results = {\n",
        "        'model': catboost_model,\n",
        "        'scaler': scaler,\n",
        "        'accuracy': accuracy,\n",
        "        'f1_macro': f1_macro,\n",
        "        'predictions': y_pred,\n",
        "        'probabilities': y_pred_proba,\n",
        "        'true_labels': y_test,\n",
        "        'feature_importance': feature_importance_df,\n",
        "        'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
        "        'classification_report': classification_report(y_test, y_pred, target_names=class_names, output_dict=True)\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "def create_catboost_visualization(model, y_true, y_pred, y_pred_proba, feature_importance_df, categories, features):\n",
        "    \"\"\"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ CatBoost\"\"\"\n",
        "\n",
        "    plt.figure(figsize=(20, 12))\n",
        "\n",
        "    # 1. Confusion Matrix\n",
        "    plt.subplot(2, 4, 1)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['SELL', 'HOLD', 'BUY'],\n",
        "                yticklabels=['SELL', 'HOLD', 'BUY'])\n",
        "    plt.title('üéØ Confusion Matrix', fontweight='bold')\n",
        "\n",
        "    # 2. Feature Importance (Top 15)\n",
        "    plt.subplot(2, 4, 2)\n",
        "    top_features = feature_importance_df.head(15)\n",
        "    plt.barh(range(len(top_features)), top_features['importance'])\n",
        "    plt.yticks(range(len(top_features)), [f[:10] for f in top_features['feature']], fontsize=8)\n",
        "    plt.title('üìä Top 15 Features', fontweight='bold')\n",
        "    plt.xlabel('Importance')\n",
        "\n",
        "    # 3. Predictions Distribution\n",
        "    plt.subplot(2, 4, 3)\n",
        "    unique, counts = np.unique(y_pred, return_counts=True)\n",
        "    class_names = ['SELL', 'HOLD', 'BUY']\n",
        "    pred_counts = [counts[unique == i][0] if i in unique else 0 for i in range(3)]\n",
        "    plt.pie(pred_counts, labels=class_names, autopct='%1.1f%%',\n",
        "            colors=['red', 'gray', 'green'], startangle=90)\n",
        "    plt.title('üéØ Predictions', fontweight='bold')\n",
        "\n",
        "    # 4. Model Confidence\n",
        "    plt.subplot(2, 4, 4)\n",
        "    confidence = np.max(y_pred_proba, axis=1)\n",
        "    plt.hist(confidence, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    plt.axvline(confidence.mean(), color='red', linestyle='--',\n",
        "                label=f'Mean: {confidence.mean():.3f}')\n",
        "    plt.title('üìä Model Confidence', fontweight='bold')\n",
        "    plt.xlabel('Max Probability')\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend()\n",
        "\n",
        "    # 5. Class Performance\n",
        "    plt.subplot(2, 4, 5)\n",
        "    from sklearn.metrics import classification_report\n",
        "    class_report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "\n",
        "    classes = ['SELL', 'HOLD', 'BUY']\n",
        "    precision_scores = [class_report[cls]['precision'] for cls in classes]\n",
        "    recall_scores = [class_report[cls]['recall'] for cls in classes]\n",
        "\n",
        "    x = np.arange(len(classes))\n",
        "    width = 0.35\n",
        "    plt.bar(x - width/2, precision_scores, width, label='Precision', alpha=0.7)\n",
        "    plt.bar(x + width/2, recall_scores, width, label='Recall', alpha=0.7)\n",
        "    plt.xlabel('Classes')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('‚öñÔ∏è Precision vs Recall', fontweight='bold')\n",
        "    plt.xticks(x, classes)\n",
        "    plt.legend()\n",
        "\n",
        "    # 6. Features by Category\n",
        "    plt.subplot(2, 4, 6)\n",
        "    category_counts = {k: len(v) for k, v in categories.items() if v}\n",
        "    plt.pie(category_counts.values(), labels=list(category_counts.keys()),\n",
        "            autopct='%1.0f%%', startangle=90)\n",
        "    plt.title('üóÇÔ∏è Features by Category', fontweight='bold')\n",
        "\n",
        "    # 7. Accuracy by Class\n",
        "    plt.subplot(2, 4, 7)\n",
        "    f1_scores = [class_report[cls]['f1-score'] for cls in classes]\n",
        "    bars = plt.bar(classes, f1_scores, color=['red', 'gray', 'green'], alpha=0.7)\n",
        "    plt.title('üìà F1-Score by Class', fontweight='bold')\n",
        "    plt.ylabel('F1-Score')\n",
        "    for bar, score in zip(bars, f1_scores):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{score:.3f}', ha='center', va='bottom')\n",
        "\n",
        "    # 8. Top Features by Category\n",
        "    plt.subplot(2, 4, 8)\n",
        "    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ç–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n",
        "    top_10_features = feature_importance_df.head(10)['feature'].tolist()\n",
        "    category_importance = {}\n",
        "\n",
        "    for category, cat_features in categories.items():\n",
        "        if cat_features:\n",
        "            importance_sum = 0\n",
        "            for feature in cat_features:\n",
        "                if feature in top_10_features:\n",
        "                    idx = feature_importance_df[feature_importance_df['feature'] == feature].index[0]\n",
        "                    importance_sum += feature_importance_df.loc[idx, 'importance']\n",
        "            if importance_sum > 0:\n",
        "                category_importance[category] = importance_sum\n",
        "\n",
        "    if category_importance:\n",
        "        cats = list(category_importance.keys())\n",
        "        values = list(category_importance.values())\n",
        "        plt.bar(range(len(cats)), values, alpha=0.7)\n",
        "        plt.xticks(range(len(cats)), [c[:8] for c in cats], rotation=45)\n",
        "        plt.title('üìä Top Categories', fontweight='bold')\n",
        "        plt.ylabel('Importance Sum')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def print_catboost_summary(results):\n",
        "    \"\"\"–ü–µ—á–∞—Ç—å –∏—Ç–æ–≥–æ–≤–æ–π —Å–≤–æ–¥–∫–∏ CatBoost\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üèÜ –ò–¢–û–ì–ò CATBOOST –ú–û–î–ï–õ–ò\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(f\"üìä –û–°–ù–û–í–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:\")\n",
        "    print(f\"   üéØ Accuracy: {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)\")\n",
        "    print(f\"   üèÜ F1-macro: {results['f1_macro']:.4f}\")\n",
        "\n",
        "    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å LSTM\n",
        "    lstm_accuracy = 0.3168  # –ò–∑ –ø—Ä–æ—à–ª—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "    improvement = (results['accuracy'] - lstm_accuracy) / lstm_accuracy * 100\n",
        "\n",
        "    print(f\"\\nüìà –°–†–ê–í–ù–ï–ù–ò–ï –° LSTM:\")\n",
        "    print(f\"   LSTM accuracy: {lstm_accuracy:.4f} (31.68%)\")\n",
        "    print(f\"   CatBoost accuracy: {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)\")\n",
        "    print(f\"   –£–ª—É—á—à–µ–Ω–∏–µ: {improvement:+.1f}%\")\n",
        "\n",
        "    if improvement > 0:\n",
        "        print(f\"   ‚úÖ CatBoost –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç LSTM –Ω–∞ {improvement:.1f}%!\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è CatBoost –Ω–µ–º–Ω–æ–≥–æ —Ö—É–∂–µ LSTM\")\n",
        "\n",
        "    print(f\"\\nüíæ –°–û–•–†–ê–ù–ï–ù–û:\")\n",
        "    print(f\"   üìÅ models/catboost_standalone.cbm\")\n",
        "    print(f\"   üìÅ models/catboost_scaler.pkl\")\n",
        "    print(f\"   üìÅ models/feature_importance.pkl\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"‚úÖ CATBOOST –ú–û–î–ï–õ–¨ –ì–û–¢–û–í–ê!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# –ó–ê–ü–£–°–ö\n",
        "print(\"üöÄ –ó–ê–ü–£–°–ö CATBOOST STANDALONE –ú–û–î–ï–õ–ò...\")\n",
        "catboost_results = train_catboost_standalone()\n",
        "print_catboost_summary(catboost_results)\n",
        "\n",
        "print(\"\\nüí° –ú–û–î–ï–õ–¨ –ì–û–¢–û–í–ê –ö –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ!\")\n",
        "print(\"üéØ CatBoost –ø–æ–∫–∞–∑–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –Ω–æ–≤—ã—Ö 63 –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YWsabrw94ftv",
        "outputId": "0b230ca2-b151-483f-f205-efee306bb7ba"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ –ó–ê–ü–£–°–ö CATBOOST STANDALONE –ú–û–î–ï–õ–ò...\n",
            "üöÄ –û–ë–£–ß–ï–ù–ò–ï CATBOOST –ú–û–î–ï–õ–ò (STANDALONE)\n",
            "============================================================\n",
            "üí° –û–±—Ö–æ–¥–∏–º –ø—Ä–æ–±–ª–µ–º—É —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å LSTM\n",
            "üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\n",
            "üîß –†–∞—Å—á–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...\n",
            "‚úÖ –†–∞—Å—Å—á–∏—Ç–∞–Ω–æ 66 –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤\n",
            "üìä –î–æ–±–∞–≤–ª–µ–Ω–æ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞: ADX, Williams %R, CCI, Stochastic\n",
            "üî• –ù–æ–≤—ã–µ —Ñ–∏—á–∏: –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è, –∫–ª–∞—Å—Ç–µ—Ä—ã, —Å–µ—Å—Å–∏–∏\n",
            "üéØ –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (–ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 60 –º–∏–Ω—É—Ç)...\n",
            "üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:\n",
            "   SELL: 1,856 (32.8%)\n",
            "   HOLD: 1,710 (30.2%)\n",
            "   BUY: 2,087 (36.9%)\n",
            "üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è LSTM...\n",
            "‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: 63 (–±—ã–ª–æ 19)\n",
            "üìä –ß–∏—Å—Ç—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: 5,587\n",
            "\n",
            "üìã –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:\n",
            "   –û—Å–Ω–æ–≤–Ω—ã–µ: 6 (Close, High, Low...)\n",
            "   –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ: 12 (SMA_9, SMA_21, EMA_12...)\n",
            "   –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ: 13 (ADX, DI_Plus, DI_Minus...)\n",
            "   –í—Ä–µ–º–µ–Ω–Ω—ã–µ: 5 (Hour, DayOfWeek, IsLondonSession...)\n",
            "   –°–∏–≥–Ω–∞–ª—ã: 11 (MA_Cross, EMA_Cross, MACD_Cross...)\n",
            "   –õ–∞–≥–∏: 10 (Returns_lag_1, Returns_lag_2, Returns_lag_3...)\n",
            "   –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è: 4 (RSI_MACD_Interaction, BB_RSI_Interaction, ADX_RSI_Strength...)\n",
            "   –ö–ª–∞—Å—Ç–µ—Ä—ã: 2 (Bullish_Cluster, Bearish_Cluster)\n",
            "\n",
            "üìä –ò—Ç–æ–≥–æ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:\n",
            "   SELL: 1,837 (32.9%)\n",
            "   HOLD: 1,681 (30.1%)\n",
            "   BUY: 2,069 (37.0%)\n",
            "üìä –î–∞–Ω–Ω—ã–µ: X(5587, 63), y(5587,)\n",
            "üî¢ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: 63\n",
            "üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:\n",
            "   Train: 3911\n",
            "   Val: 838\n",
            "   Test: 838\n",
            "\n",
            "üöÄ –û–±—É—á–µ–Ω–∏–µ CatBoost –º–æ–¥–µ–ª–∏...\n",
            "0:\tlearn: 1.0940618\ttest: 1.0956832\tbest: 1.0956832 (0)\ttotal: 32.9ms\tremaining: 32.8s\n",
            "Stopped by overfitting detector  (50 iterations wait)\n",
            "\n",
            "bestTest = 1.093791006\n",
            "bestIteration = 26\n",
            "\n",
            "Shrink model to first 27 iterations.\n",
            "‚úÖ CatBoost –æ–±—É—á–µ–Ω!\n",
            "\n",
            "üìä –¢–û–ü-15 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\n",
            "   Hour: 5.9\n",
            "   Volatility_5: 5.8\n",
            "   Low: 5.4\n",
            "   IsNYSession: 4.7\n",
            "   ATR_Ratio: 4.6\n",
            "   MACD_Signal: 3.7\n",
            "   Vol_Ratio: 3.4\n",
            "   BB_Position: 3.3\n",
            "   ADX: 3.3\n",
            "   Volatility_20: 3.1\n",
            "   High: 2.9\n",
            "   DayOfWeek: 2.7\n",
            "   EMA_26: 2.7\n",
            "   ATR_Percent: 2.7\n",
            "   SMA_21: 2.6\n",
            "\n",
            "ü§ñ –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...\n",
            "\n",
            "üéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ CATBOOST:\n",
            "   Accuracy: 0.2589 (25.89%)\n",
            "   F1-macro: 0.1290\n",
            "\n",
            "üìã –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Number of classes, 4, does not match size of target_names, 3. Try specifying the labels parameter",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-978519610>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;31m# –ó–ê–ü–£–°–ö\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üöÄ –ó–ê–ü–£–°–ö CATBOOST STANDALONE –ú–û–î–ï–õ–ò...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m \u001b[0mcatboost_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_catboost_standalone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0mprint_catboost_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcatboost_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-102-978519610>\u001b[0m in \u001b[0;36mtrain_catboost_standalone\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'SELL'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HOLD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'BUY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nüìã –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# 8. SHAP –∞–Ω–∞–ª–∏–∑\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2691\u001b[0m             )\n\u001b[1;32m   2692\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2694\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Number of classes, 4, does not match size of target_names, 3. Try specifying the labels parameter"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –í –∫–æ–Ω—Ü–µ –Ω–æ–≤–æ–≥–æ –∫–æ–¥–∞ –µ—Å—Ç—å:\n",
        "catboost_results = train_catboost_standalone()  # ‚Üê –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M2nDqTqCZ-vT",
        "outputId": "231c2eba-67d1-4528-b2be-4858087d6ea4"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ –û–ë–£–ß–ï–ù–ò–ï CATBOOST –ú–û–î–ï–õ–ò (STANDALONE)\n",
            "============================================================\n",
            "üí° –û–±—Ö–æ–¥–∏–º –ø—Ä–æ–±–ª–µ–º—É —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å LSTM\n",
            "üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\n",
            "üîß –†–∞—Å—á–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...\n",
            "‚úÖ –†–∞—Å—Å—á–∏—Ç–∞–Ω–æ 66 –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤\n",
            "üìä –î–æ–±–∞–≤–ª–µ–Ω–æ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞: ADX, Williams %R, CCI, Stochastic\n",
            "üî• –ù–æ–≤—ã–µ —Ñ–∏—á–∏: –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è, –∫–ª–∞—Å—Ç–µ—Ä—ã, —Å–µ—Å—Å–∏–∏\n",
            "üéØ –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (–ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 60 –º–∏–Ω—É—Ç)...\n",
            "üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:\n",
            "   SELL: 1,856 (32.8%)\n",
            "   HOLD: 1,710 (30.2%)\n",
            "   BUY: 2,087 (36.9%)\n",
            "üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è LSTM...\n",
            "‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: 63 (–±—ã–ª–æ 19)\n",
            "üìä –ß–∏—Å—Ç—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: 5,587\n",
            "\n",
            "üìã –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:\n",
            "   –û—Å–Ω–æ–≤–Ω—ã–µ: 6 (Close, High, Low...)\n",
            "   –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ: 12 (SMA_9, SMA_21, EMA_12...)\n",
            "   –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ: 13 (ADX, DI_Plus, DI_Minus...)\n",
            "   –í—Ä–µ–º–µ–Ω–Ω—ã–µ: 5 (Hour, DayOfWeek, IsLondonSession...)\n",
            "   –°–∏–≥–Ω–∞–ª—ã: 11 (MA_Cross, EMA_Cross, MACD_Cross...)\n",
            "   –õ–∞–≥–∏: 10 (Returns_lag_1, Returns_lag_2, Returns_lag_3...)\n",
            "   –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è: 4 (RSI_MACD_Interaction, BB_RSI_Interaction, ADX_RSI_Strength...)\n",
            "   –ö–ª–∞—Å—Ç–µ—Ä—ã: 2 (Bullish_Cluster, Bearish_Cluster)\n",
            "\n",
            "üìä –ò—Ç–æ–≥–æ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:\n",
            "   SELL: 1,837 (32.9%)\n",
            "   HOLD: 1,681 (30.1%)\n",
            "   BUY: 2,069 (37.0%)\n",
            "üìä –î–∞–Ω–Ω—ã–µ: X(5587, 63), y(5587,)\n",
            "üî¢ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: 63\n",
            "üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:\n",
            "   Train: 3911\n",
            "   Val: 838\n",
            "   Test: 838\n",
            "\n",
            "üöÄ –û–±—É—á–µ–Ω–∏–µ CatBoost –º–æ–¥–µ–ª–∏...\n",
            "0:\tlearn: 1.0940618\ttest: 1.0956832\tbest: 1.0956832 (0)\ttotal: 64.1ms\tremaining: 1m 4s\n",
            "Stopped by overfitting detector  (50 iterations wait)\n",
            "\n",
            "bestTest = 1.093791006\n",
            "bestIteration = 26\n",
            "\n",
            "Shrink model to first 27 iterations.\n",
            "‚úÖ CatBoost –æ–±—É—á–µ–Ω!\n",
            "\n",
            "üìä –¢–û–ü-15 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\n",
            "   Hour: 5.9\n",
            "   Volatility_5: 5.8\n",
            "   Low: 5.4\n",
            "   IsNYSession: 4.7\n",
            "   ATR_Ratio: 4.6\n",
            "   MACD_Signal: 3.7\n",
            "   Vol_Ratio: 3.4\n",
            "   BB_Position: 3.3\n",
            "   ADX: 3.3\n",
            "   Volatility_20: 3.1\n",
            "   High: 2.9\n",
            "   DayOfWeek: 2.7\n",
            "   EMA_26: 2.7\n",
            "   ATR_Percent: 2.7\n",
            "   SMA_21: 2.6\n",
            "\n",
            "ü§ñ –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...\n",
            "\n",
            "üéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ CATBOOST:\n",
            "   Accuracy: 0.2589 (25.89%)\n",
            "   F1-macro: 0.1290\n",
            "\n",
            "üìã –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Number of classes, 4, does not match size of target_names, 3. Try specifying the labels parameter",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-2770428747>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# –í –∫–æ–Ω—Ü–µ –Ω–æ–≤–æ–≥–æ –∫–æ–¥–∞ –µ—Å—Ç—å:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcatboost_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_catboost_standalone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ‚Üê –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-102-978519610>\u001b[0m in \u001b[0;36mtrain_catboost_standalone\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'SELL'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HOLD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'BUY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nüìã –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# 8. SHAP –∞–Ω–∞–ª–∏–∑\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2691\u001b[0m             )\n\u001b[1;32m   2692\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2694\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Number of classes, 4, does not match size of target_names, 3. Try specifying the labels parameter"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}